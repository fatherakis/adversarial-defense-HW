{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hNNUd76x2YL",
        "outputId": "62dab4f1-c098-4e82-8ebc-deac107f7105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2023-12-18T10:59:15.788631Z",
          "iopub.status.busy": "2023-12-18T10:59:15.787823Z",
          "iopub.status.idle": "2023-12-18T10:59:56.425178Z",
          "shell.execute_reply": "2023-12-18T10:59:56.424048Z",
          "shell.execute_reply.started": "2023-12-18T10:59:15.788586Z"
        },
        "id": "AWUJn5D1x1nn",
        "outputId": "14c6f18c-786f-4ebd-c2af-d190db2ef720",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n",
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.17.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.11.4)\n",
            "Collecting scikit-learn<1.2.0,>=0.22.2 (from adversarial-robustness-toolbox)\n",
            "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (4.66.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (3.2.0)\n",
            "Installing collected packages: scikit-learn, adversarial-robustness-toolbox\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.20.1 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed adversarial-robustness-toolbox-1.17.0 scikit-learn-1.1.3\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install matplotlib colorama\n",
        "!pip3 install adversarial-robustness-toolbox\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-18T10:59:56.428163Z",
          "iopub.status.busy": "2023-12-18T10:59:56.427843Z",
          "iopub.status.idle": "2023-12-18T11:00:01.443317Z",
          "shell.execute_reply": "2023-12-18T11:00:01.442254Z",
          "shell.execute_reply.started": "2023-12-18T10:59:56.428136Z"
        },
        "id": "eTw0TbFKx1nu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.quantization import MovingAverageMinMaxObserver\n",
        "from torch.ao.quantization.observer import MinMaxObserver\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "from colorama import Fore, Style\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "from art.utils import load_cifar10\n",
        "from art.preprocessing.standardisation_mean_std import StandardisationMeanStdPyTorch\n",
        "from art.attacks.evasion.hop_skip_jump import HopSkipJump\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "from math import log10, sqrt\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "def accuracy(output, target):\n",
        "    \"\"\" Computes the top 1 accuracy \"\"\"\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        correct_one = correct[:1].view(-1).float().sum(0, keepdim=True)\n",
        "        return correct_one.mul_(100.0 / batch_size).item()\n",
        "\n",
        "def predict(img):\n",
        "  img = torch.from_numpy(img)\n",
        "  img = F.normalize(img, [0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
        "  return img.numpy()\n",
        "\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    \"\"\" Prints the real size of the model \"\"\"\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "def train(model: nn.Module, dataloader: DataLoader, cuda=False, q=False):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)#, momentum=0.9, dampening=0,weight_decay=0.0005, nesterov=True)\n",
        "    model.train()\n",
        "    for epoch in range(1):  # loop over the dataset multiple times\n",
        "        running_loss = AverageMeter('loss')\n",
        "        acc = AverageMeter('train_acc')\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            if epoch>=3 and q:\n",
        "              model.apply(torch.quantization.disable_observer)\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print statistics\n",
        "            running_loss.update(loss.item(), outputs.shape[0])\n",
        "            acc.update(accuracy(outputs, labels), outputs.shape[0])\n",
        "            if i % 100 == 0:    # print every 100 mini-batches\n",
        "                print('[%d, %5d] ' %\n",
        "                    (epoch + 1, i + 1), running_loss, acc)\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "def test(model: nn.Module, dataloader: DataLoader, cuda=False) -> float:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-12-18T11:00:01.445395Z",
          "iopub.status.busy": "2023-12-18T11:00:01.444683Z",
          "iopub.status.idle": "2023-12-18T11:00:17.908485Z",
          "shell.execute_reply": "2023-12-18T11:00:17.907488Z",
          "shell.execute_reply.started": "2023-12-18T11:00:01.445356Z"
        },
        "id": "JifZQy1Qx1nw",
        "outputId": "3dcbbfc1-0d89-45bf-dde1-5d26385395e0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26421880/26421880 [00:00<00:00, 111553859.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 7017452.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 4422102/4422102 [00:00<00:00, 54175363.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 19487614.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "image_set = pickle.load(open(\"/content/drive/MyDrive/Diplomatiki/FashionMnist_set/fashionmnist_images.pkl\",'rb'))\n",
        "label_set = pickle.load(open(\"/content/drive/MyDrive/Diplomatiki/FashionMnist_set/fashionmnist_label.pkl\",'rb'))\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MxQHJYiKFRq"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from functools import partial\n",
        "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
        "from torch.ao.nn.quantized.modules.functional_modules import FloatFunctional\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.ff = torch.nn.quantized.FloatFunctional()\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        #out += identity\n",
        "        out = self.ff.add(out, identity)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(CifarResNet, self).__init__()\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = conv3x3(1, 16)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = CifarResNet(BasicBlock,[9]*3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TueUKyy2XlHT"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
        "from torch import Tensor\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from torch.ao.nn.quantized.modules.functional_modules import FloatFunctional\n",
        "\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        f_add = torch.nn.quantized.FloatFunctional()\n",
        "        new_v = f_add.add(new_v, divisor)\n",
        "        #new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class ConvBNActivation(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_planes: int,\n",
        "        out_planes: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        dilation: int = 1,\n",
        "    ) -> None:\n",
        "        padding = (kernel_size - 1) // 2 * dilation\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if activation_layer is None:\n",
        "            activation_layer = nn.ReLU6\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups,\n",
        "                      bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            activation_layer(inplace=True)\n",
        "        )\n",
        "        self.out_channels = out_planes\n",
        "\n",
        "\n",
        "# necessary for backwards compatibility\n",
        "ConvBNReLU = ConvBNActivation\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp: int,\n",
        "        oup: int,\n",
        "        stride: int,\n",
        "        expand_ratio: int,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "        if expand_ratio != 1:\n",
        "            # pw\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n",
        "        layers.extend([\n",
        "            # dw\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n",
        "            # pw-linear\n",
        "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "            norm_layer(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        self.out_channels = oup\n",
        "        self._is_cn = stride > 1\n",
        "        self.ff = torch.nn.quantized.FloatFunctional()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.use_res_connect:\n",
        "            #return x + self.conv(x)\n",
        "            return self.ff.add(x, self.conv(x))\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 10,\n",
        "        width_mult: float = 1.0,\n",
        "        inverted_residual_setting: Optional[List[List[int]]] = None,\n",
        "        round_nearest: int = 8,\n",
        "        block: Optional[Callable[..., nn.Module]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        MobileNet V2 main class\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes\n",
        "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
        "            inverted_residual_setting: Network structure\n",
        "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
        "            Set to 1 to turn off rounding\n",
        "            block: Module specifying inverted residual building block for mobilenet\n",
        "            norm_layer: Module specifying the normalization layer to use\n",
        "\n",
        "        \"\"\"\n",
        "        super(MobileNetV2, self).__init__()\n",
        "\n",
        "        if block is None:\n",
        "            block = InvertedResidual\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        if inverted_residual_setting is None:\n",
        "            inverted_residual_setting = [\n",
        "                # t, c, n, s\n",
        "                [1, 16, 1, 1],\n",
        "                [6, 24, 2, 1],  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
        "                [6, 32, 3, 2],\n",
        "                [6, 64, 4, 2],\n",
        "                [6, 96, 3, 1],\n",
        "                [6, 160, 3, 2],\n",
        "                [6, 320, 1, 1],\n",
        "            ]\n",
        "\n",
        "        # only check the first element, assuming user knows t,c,n,s are required\n",
        "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
        "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
        "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
        "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
        "        features: List[nn.Module] = [ConvBNReLU(1, input_channel, stride=1, norm_layer=norm_layer)]  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # This exists since TorchScript doesn't support inheritance, so the superclass method\n",
        "        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n",
        "        x = self.features(x)\n",
        "        # Cannot use \"squeeze\" as batch-size can be 1\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "model = MobileNetV2(width_mult = 1.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-12-18T11:51:38.584818Z",
          "iopub.status.busy": "2023-12-18T11:51:38.583951Z",
          "iopub.status.idle": "2023-12-18T11:58:42.092927Z",
          "shell.execute_reply": "2023-12-18T11:58:42.091631Z",
          "shell.execute_reply.started": "2023-12-18T11:51:38.584782Z"
        },
        "id": "Ec0OO_ZUx1nx",
        "outputId": "29f536e6-e47e-4d34-8107-fa3b9816f032",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 93.28% - FP32\n",
            "(10000, 1, 28, 28) (10000,) 3\n",
            "[1,     1]  loss 0.042340 (0.042340) train_acc 98.437500 (98.437500)\n",
            "[1,   101]  loss 0.042839 (0.023616) train_acc 98.437500 (99.443069)\n",
            "Accuracy of the network on the test images: 93.17% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[2,     1]  loss 0.033011 (0.033011) train_acc 98.437500 (98.437500)\n",
            "[2,   101]  loss 0.028462 (0.022135) train_acc 98.437500 (99.474010)\n",
            "Accuracy of the network on the test images: 93.2% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[3,     1]  loss 0.023110 (0.023110) train_acc 100.000000 (100.000000)\n",
            "[3,   101]  loss 0.032415 (0.018698) train_acc 100.000000 (99.690594)\n",
            "Accuracy of the network on the test images: 93.11% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[4,     1]  loss 0.028364 (0.028364) train_acc 98.437500 (98.437500)\n",
            "[4,   101]  loss 0.035503 (0.022428) train_acc 98.437500 (99.458540)\n",
            "Accuracy of the network on the test images: 93.12% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[5,     1]  loss 0.009809 (0.009809) train_acc 100.000000 (100.000000)\n",
            "[5,   101]  loss 0.018733 (0.018708) train_acc 100.000000 (99.613243)\n",
            "Accuracy of the network on the test images: 93.19% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[6,     1]  loss 0.015491 (0.015491) train_acc 100.000000 (100.000000)\n",
            "[6,   101]  loss 0.006224 (0.019358) train_acc 100.000000 (99.566832)\n",
            "Accuracy of the network on the test images: 93.26% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[7,     1]  loss 0.022972 (0.022972) train_acc 98.437500 (98.437500)\n",
            "[7,   101]  loss 0.017293 (0.016143) train_acc 100.000000 (99.752475)\n",
            "Accuracy of the network on the test images: 93.26% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[8,     1]  loss 0.004852 (0.004852) train_acc 100.000000 (100.000000)\n",
            "[8,   101]  loss 0.018191 (0.017869) train_acc 100.000000 (99.566832)\n",
            "Accuracy of the network on the test images: 93.24% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[9,     1]  loss 0.025270 (0.025270) train_acc 100.000000 (100.000000)\n",
            "[9,   101]  loss 0.006505 (0.013841) train_acc 100.000000 (99.845297)\n",
            "Accuracy of the network on the test images: 93.2% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[10,     1]  loss 0.006046 (0.006046) train_acc 100.000000 (100.000000)\n",
            "[10,   101]  loss 0.006428 (0.014929) train_acc 100.000000 (99.721535)\n",
            "Accuracy of the network on the test images: 93.13% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[11,     1]  loss 0.020064 (0.020064) train_acc 100.000000 (100.000000)\n",
            "[11,   101]  loss 0.033465 (0.015023) train_acc 98.437500 (99.659653)\n",
            "Accuracy of the network on the test images: 93.15% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[12,     1]  loss 0.024028 (0.024028) train_acc 98.437500 (98.437500)\n",
            "[12,   101]  loss 0.012424 (0.013802) train_acc 100.000000 (99.737005)\n",
            "Accuracy of the network on the test images: 93.26% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[13,     1]  loss 0.023275 (0.023275) train_acc 98.437500 (98.437500)\n",
            "[13,   101]  loss 0.008109 (0.013156) train_acc 100.000000 (99.767946)\n",
            "Accuracy of the network on the test images: 93.23% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[14,     1]  loss 0.013029 (0.013029) train_acc 100.000000 (100.000000)\n",
            "[14,   101]  loss 0.012143 (0.013001) train_acc 100.000000 (99.814356)\n",
            "Accuracy of the network on the test images: 93.22% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[15,     1]  loss 0.008650 (0.008650) train_acc 100.000000 (100.000000)\n",
            "[15,   101]  loss 0.006624 (0.012859) train_acc 100.000000 (99.737005)\n",
            "Accuracy of the network on the test images: 93.11% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[16,     1]  loss 0.023567 (0.023567) train_acc 100.000000 (100.000000)\n",
            "[16,   101]  loss 0.005602 (0.013295) train_acc 100.000000 (99.690594)\n",
            "Accuracy of the network on the test images: 93.16% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[17,     1]  loss 0.003635 (0.003635) train_acc 100.000000 (100.000000)\n",
            "[17,   101]  loss 0.003907 (0.011097) train_acc 100.000000 (99.783416)\n",
            "Accuracy of the network on the test images: 93.13% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[18,     1]  loss 0.019053 (0.019053) train_acc 100.000000 (100.000000)\n",
            "[18,   101]  loss 0.006452 (0.011646) train_acc 100.000000 (99.845297)\n",
            "Accuracy of the network on the test images: 93.26% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[19,     1]  loss 0.002353 (0.002353) train_acc 100.000000 (100.000000)\n",
            "[19,   101]  loss 0.003660 (0.012049) train_acc 100.000000 (99.798886)\n",
            "Accuracy of the network on the test images: 93.2% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[20,     1]  loss 0.012720 (0.012720) train_acc 100.000000 (100.000000)\n",
            "[20,   101]  loss 0.007355 (0.009993) train_acc 100.000000 (99.876238)\n",
            "Accuracy of the network on the test images: 93.15% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[21,     1]  loss 0.010964 (0.010964) train_acc 100.000000 (100.000000)\n",
            "[21,   101]  loss 0.003261 (0.011561) train_acc 100.000000 (99.721535)\n",
            "Accuracy of the network on the test images: 93.14% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[22,     1]  loss 0.019225 (0.019225) train_acc 100.000000 (100.000000)\n",
            "[22,   101]  loss 0.005993 (0.010043) train_acc 100.000000 (99.798886)\n",
            "Accuracy of the network on the test images: 93.18% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[23,     1]  loss 0.006362 (0.006362) train_acc 100.000000 (100.000000)\n",
            "[23,   101]  loss 0.008625 (0.009072) train_acc 100.000000 (99.891708)\n",
            "Accuracy of the network on the test images: 93.27% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[24,     1]  loss 0.008235 (0.008235) train_acc 100.000000 (100.000000)\n",
            "[24,   101]  loss 0.007761 (0.009466) train_acc 100.000000 (99.845297)\n",
            "Accuracy of the network on the test images: 93.19% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[25,     1]  loss 0.005632 (0.005632) train_acc 100.000000 (100.000000)\n",
            "[25,   101]  loss 0.005228 (0.010527) train_acc 100.000000 (99.767946)\n",
            "Accuracy of the network on the test images: 93.12% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[26,     1]  loss 0.004361 (0.004361) train_acc 100.000000 (100.000000)\n",
            "[26,   101]  loss 0.008419 (0.009445) train_acc 100.000000 (99.845297)\n",
            "Accuracy of the network on the test images: 93.13% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[27,     1]  loss 0.015531 (0.015531) train_acc 100.000000 (100.000000)\n",
            "[27,   101]  loss 0.021827 (0.008709) train_acc 98.437500 (99.876238)\n",
            "Accuracy of the network on the test images: 93.17% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[28,     1]  loss 0.002386 (0.002386) train_acc 100.000000 (100.000000)\n",
            "[28,   101]  loss 0.002712 (0.007962) train_acc 100.000000 (99.922649)\n",
            "Accuracy of the network on the test images: 93.26% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[29,     1]  loss 0.003422 (0.003422) train_acc 100.000000 (100.000000)\n",
            "[29,   101]  loss 0.005795 (0.007809) train_acc 100.000000 (99.938119)\n",
            "Accuracy of the network on the test images: 93.28% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[30,     1]  loss 0.002234 (0.002234) train_acc 100.000000 (100.000000)\n",
            "[30,   101]  loss 0.011240 (0.008327) train_acc 100.000000 (99.860767)\n",
            "Accuracy of the network on the test images: 93.22% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[31,     1]  loss 0.002700 (0.002700) train_acc 100.000000 (100.000000)\n",
            "[31,   101]  loss 0.005802 (0.008723) train_acc 100.000000 (99.891708)\n",
            "Accuracy of the network on the test images: 93.1% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[32,     1]  loss 0.017478 (0.017478) train_acc 100.000000 (100.000000)\n",
            "[32,   101]  loss 0.002944 (0.008421) train_acc 100.000000 (99.845297)\n",
            "Accuracy of the network on the test images: 93.21% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "[33,     1]  loss 0.003706 (0.003706) train_acc 100.000000 (100.000000)\n",
            "[33,   101]  loss 0.001910 (0.007277) train_acc 100.000000 (99.922649)\n",
            "Accuracy of the network on the test images: 93.31% - FP32\n",
            "Accuracy of the network on the adversarial set images: 100.0% - FP32\n",
            "Finished Training\n",
            "Accuracy of the network on the test images: 93.31% - FP32\n"
          ]
        }
      ],
      "source": [
        "def train(model: nn.Module, dataloader: DataLoader,testloader: DataLoader, cuda=False, q=False):\n",
        "    flagged = False\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-6)#optim.SGD(model.parameters(), lr=0.000001, momentum=0.9, dampening=0,weight_decay=0.0005, nesterov=True)#\n",
        "    model.train()\n",
        "    for epoch in range(200):  # loop over the dataset multiple times\n",
        "        if flagged:\n",
        "            break\n",
        "        else:\n",
        "            running_loss = AverageMeter('loss')\n",
        "            acc = AverageMeter('train_acc')\n",
        "            for i, data in enumerate(dataloader, 0):\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                inputs, labels = data\n",
        "                if cuda:\n",
        "                  inputs = inputs.cuda()\n",
        "                  labels = labels.cuda()\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                if epoch>=3 and q:\n",
        "                  model.apply(torch.quantization.disable_observer)\n",
        "                # forward + backward + optimize\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                # print statistics\n",
        "                running_loss.update(loss.item(), outputs.shape[0])\n",
        "                acc.update(accuracy(outputs, labels), outputs.shape[0])\n",
        "                if i % 100 == 0:    # print every 100 mini-batches\n",
        "                    print('[%d, %5d] ' %\n",
        "                        (epoch + 1, i + 1), running_loss, acc)\n",
        "            score = test(model, testloader, cuda=True)\n",
        "            print('Accuracy of the network on the test images: {}% - FP32'.format(score))\n",
        "            score1 = test(model, dataloader, cuda=True)\n",
        "            print('Accuracy of the network on the adversarial set images: {}% - FP32'.format(score1))\n",
        "            if (score >= 93.30 and score1 >= 99.7):\n",
        "                flagged = True\n",
        "                #pass\n",
        "            model.train()\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/MobileNet_3it_Fashion_93.28acc.pkl\"))\n",
        "\n",
        "model.to(\"cuda\")\n",
        "model.eval()\n",
        "score = test(model, testloader, cuda=True)\n",
        "print('Accuracy of the network on the test images: {}% - FP32'.format(score))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "file = open(\"/content/drive/MyDrive/Thesis/MobileNetv2_x1_4/FashionMNIST/Train Iteration 3/train_mobilenet_hop_fashion_for_3it.pkl\",'rb')\n",
        "x_train_set = pickle.load(file)\n",
        "\n",
        "sample_size = 10000\n",
        "\n",
        "x_train_set = x_train_set[0:sample_size]\n",
        "#print(x_train_set.shape)\n",
        "y_train_set = label_set[0:sample_size]\n",
        "\n",
        "print(x_train_set.shape, y_train_set.shape, y_train_set[0])\n",
        "\n",
        "\n",
        "trainingSet = torch.utils.data.TensorDataset(torch.from_numpy(x_train_set).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "\n",
        "train_dev_sets = torch.utils.data.ConcatDataset([trainingSet])#, trainOriginalSet])\n",
        "def custom_collate(batch):\n",
        "    # Combine a list of samples into a batch\n",
        "    data, labels = zip(*batch)\n",
        "    data = torch.stack(data)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "retrain_loader = torch.utils.data.DataLoader(dataset=train_dev_sets, batch_size=64, num_workers=16, pin_memory=True, shuffle=True, collate_fn=custom_collate)\n",
        "\n",
        "train(model,retrain_loader,testloader,cuda=True,q=False)\n",
        "\n",
        "score = test(model, testloader, cuda=True)\n",
        "print('Accuracy of the network on the test images: {}% - FP32'.format(score))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w9yLdG7WqJw",
        "outputId": "8fc191fe-511b-407f-c4ff-db18a0833831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 92.94% - FP32\n",
            "Accuracy of the network on the train images: 99.95% - FP32\n"
          ]
        }
      ],
      "source": [
        "score = test(model, testloader, cuda=True)\n",
        "print('Accuracy of the network on the test images: {}% - FP32'.format(score))\n",
        "score1 = test(model, retrain_loader, cuda=True)\n",
        "print('Accuracy of the network on the train images: {}% - FP32'.format(score1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-12-18T11:58:42.356906Z",
          "iopub.status.busy": "2023-12-18T11:58:42.356453Z",
          "iopub.status.idle": "2023-12-18T11:58:42.374650Z",
          "shell.execute_reply": "2023-12-18T11:58:42.373795Z",
          "shell.execute_reply.started": "2023-12-18T11:58:42.356881Z"
        },
        "id": "IhwVrYK5x1ny",
        "outputId": "22b56715-c03a-4fc8-d696-46f05a8d68cc",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 93.31% - FP32\n"
          ]
        }
      ],
      "source": [
        "score = test(model, testloader, cuda=True)\n",
        "print('Accuracy of the network on the test images: {}% - FP32'.format(score))\n",
        "torch.save(model.state_dict(), \"MobileNet_3it_Fashion_{}acc.pkl\".format(score))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 3646861,
          "sourceId": 7182625,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4152726,
          "sourceId": 7183909,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4151988,
          "sourceId": 7229385,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30528,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
