{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcclHiU5S94D",
        "outputId": "5db4b82d-7682-4bf5-d6d2-5e9d4cda2ef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.17.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (4.66.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.3.0)\n",
            "Installing collected packages: adversarial-robustness-toolbox\n",
            "Successfully installed adversarial-robustness-toolbox-1.17.1\n"
          ]
        }
      ],
      "source": [
        "!pip install adversarial-robustness-toolbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BAT_Np0S83v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import time\n",
        "from math import log10, sqrt\n",
        "from torch.utils.data import DataLoader\n",
        "from art.utils import load_cifar10\n",
        "import random\n",
        "from torch.quantization import MovingAverageMinMaxObserver\n",
        "from torch.ao.quantization.observer import MinMaxObserver\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQLrFqWZcmDY",
        "outputId": "13d6ce68-1f4b-4d17-e156-46000f3469ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 71751710.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "def test(model: nn.Module, dataloader: DataLoader, cuda=False) -> float:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def predict(img):\n",
        "  img = torch.from_numpy(img)\n",
        "  img = F.normalize(img, [0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
        "  return img.numpy()\n",
        "\n",
        "def custom_collate(batch):\n",
        "        # Combine a list of samples into a batch\n",
        "        data, labels = zip(*batch)\n",
        "        data = torch.stack(data)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "        return data, labels\n",
        "\n",
        "def evaluator(model, loader):\n",
        "  model.eval()\n",
        "  top_1 = 0\n",
        "  top_5 = 0\n",
        "  with torch.no_grad():\n",
        "    for data in loader:\n",
        "      inputs, labels = data\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      _, predicted = torch.max(outputs, 1, keepdim=True)\n",
        "      top_1 += torch.sum(predicted.view(-1) == labels).item()\n",
        "\n",
        "      _, predicted_5 = torch.topk(outputs, k=5, dim=1)\n",
        "      top_5 += torch.sum(predicted_5 == labels.unsqueeze(1)).item()\n",
        "\n",
        "  return (\"{:.2f}\".format((top_1/400) * 100), \"{:.2f}\".format((top_5/400) * 100))\n",
        "\n",
        "def evaluator_testset(model, loader):\n",
        "  model.eval()\n",
        "  top_1 = 0\n",
        "  top_5 = 0\n",
        "  with torch.no_grad():\n",
        "    for data in loader:\n",
        "      inputs, labels = data\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      _, predicted = torch.max(outputs, 1, keepdim=True)\n",
        "      top_1 += torch.sum(predicted.view(-1) == labels).item()\n",
        "\n",
        "      _, predicted_5 = torch.topk(outputs, k=5, dim=1)\n",
        "      top_5 += torch.sum(predicted_5 == labels.unsqueeze(1)).item()\n",
        "\n",
        "  return (\"{:.2f}\".format((top_1/10000) * 100), \"{:.2f}\".format((top_5/10000) * 100))\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Thesis/FashionMnist_set/testset.pkl\", 'rb') as f:\n",
        "  fashion_testset = pickle.load(f)\n",
        "\n",
        "fashion_testloader = torch.utils.data.DataLoader(fashion_testset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIZa4x6LQj_E"
      },
      "outputs": [],
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = load_cifar10()\n",
        "\n",
        "x_train = np.transpose(x_train, (0, 3, 1, 2)).astype(np.float32)\n",
        "x_test = np.transpose(x_test, (0, 3, 1, 2)).astype(np.float32)\n",
        "\n",
        "classes_cifar = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "classes_fashion = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle-boot']\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Thesis/FashionMnist_set/fashionmnist_label.pkl\",'rb') as f:\n",
        "  y_f_train_set = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Thesis/FashionMnist_set/fashionmnist_test_label.pkl\",'rb') as f:\n",
        "  y_f_test_set = pickle.load(f)\n",
        "\n",
        "y_f_train_set = y_f_train_set[0:400]\n",
        "\n",
        "y_f_test_set = y_f_test_set[0:400]\n",
        "\n",
        "y_test_set = np.zeros((400,),np.int8)\n",
        "\n",
        "y_train_set = np.zeros((400,),np.int8)\n",
        "\n",
        "\n",
        "for i in range(400):\n",
        "        y_test_set[i] = np.where(y_test[i] == 1)[0][0]\n",
        "\n",
        "for i in range(400):\n",
        "        y_train_set[i] = np.where(y_train[i] == 1)[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTh24FhMYC28"
      },
      "source": [
        "## CIFAR-10 ResNet Quantized Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IL8voP3tYBCi"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from functools import partial\n",
        "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
        "from torch.ao.nn.quantized.modules.functional_modules import FloatFunctional\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.ff = torch.nn.quantized.FloatFunctional()\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        #out += identity\n",
        "        out = self.ff.add(out, identity)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(CifarResNet, self).__init__()\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = conv3x3(3, 16)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRSPVYrZa-n6"
      },
      "source": [
        "## CIFAR-10 MobileNet Quantized Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTyn6pfmbRTW"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
        "from torch import Tensor\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from torch.ao.nn.quantized.modules.functional_modules import FloatFunctional\n",
        "\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        f_add = torch.nn.quantized.FloatFunctional()\n",
        "        new_v = f_add.add(new_v, divisor)\n",
        "        #new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class ConvBNActivation(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_planes: int,\n",
        "        out_planes: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        dilation: int = 1,\n",
        "    ) -> None:\n",
        "        padding = (kernel_size - 1) // 2 * dilation\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if activation_layer is None:\n",
        "            activation_layer = nn.ReLU6\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups,\n",
        "                      bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            activation_layer(inplace=True)\n",
        "        )\n",
        "        self.out_channels = out_planes\n",
        "\n",
        "\n",
        "# necessary for backwards compatibility\n",
        "ConvBNReLU = ConvBNActivation\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp: int,\n",
        "        oup: int,\n",
        "        stride: int,\n",
        "        expand_ratio: int,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "        if expand_ratio != 1:\n",
        "            # pw\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n",
        "        layers.extend([\n",
        "            # dw\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n",
        "            # pw-linear\n",
        "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "            norm_layer(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        self.out_channels = oup\n",
        "        self._is_cn = stride > 1\n",
        "        self.ff = torch.nn.quantized.FloatFunctional()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.use_res_connect:\n",
        "            #return x + self.conv(x)\n",
        "            return self.ff.add(x, self.conv(x))\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 10,\n",
        "        width_mult: float = 1.0,\n",
        "        inverted_residual_setting: Optional[List[List[int]]] = None,\n",
        "        round_nearest: int = 8,\n",
        "        block: Optional[Callable[..., nn.Module]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        MobileNet V2 main class\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes\n",
        "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
        "            inverted_residual_setting: Network structure\n",
        "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
        "            Set to 1 to turn off rounding\n",
        "            block: Module specifying inverted residual building block for mobilenet\n",
        "            norm_layer: Module specifying the normalization layer to use\n",
        "\n",
        "        \"\"\"\n",
        "        super(MobileNetV2, self).__init__()\n",
        "\n",
        "        if block is None:\n",
        "            block = InvertedResidual\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        if inverted_residual_setting is None:\n",
        "            inverted_residual_setting = [\n",
        "                # t, c, n, s\n",
        "                [1, 16, 1, 1],\n",
        "                [6, 24, 2, 1],  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
        "                [6, 32, 3, 2],\n",
        "                [6, 64, 4, 2],\n",
        "                [6, 96, 3, 1],\n",
        "                [6, 160, 3, 2],\n",
        "                [6, 320, 1, 1],\n",
        "            ]\n",
        "\n",
        "        # only check the first element, assuming user knows t,c,n,s are required\n",
        "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
        "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
        "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
        "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
        "        features: List[nn.Module] = [ConvBNReLU(3, input_channel, stride=1, norm_layer=norm_layer)]  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # This exists since TorchScript doesn't support inheritance, so the superclass method\n",
        "        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n",
        "        x = self.features(x)\n",
        "        # Cannot use \"squeeze\" as batch-size can be 1\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGBDJRVEccJA"
      },
      "source": [
        "## Model Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_AOtzk0dkIU",
        "outputId": "e9cb126b-fdfc-40fc-d243-4c21e7a2cca6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuantWrapper(\n",
              "  (quant): Quantize(scale=tensor([0.0203]), zero_point=tensor([120]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              "  (module): CifarResNet(\n",
              "    (conv1): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.05107611045241356, zero_point=126, padding=(1, 1), bias=False)\n",
              "    (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.014130653813481331, zero_point=169, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.010054154321551323, zero_point=122, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.0206138975918293, zero_point=100\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.019558319821953773, zero_point=163, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.008089272305369377, zero_point=93, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.01972598023712635, zero_point=71\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.03201309219002724, zero_point=139, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.011237370781600475, zero_point=114, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.023366838693618774, zero_point=84\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.03337525576353073, zero_point=146, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.013408266007900238, zero_point=128, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), scale=0.010232594795525074, zero_point=132, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.020983587950468063, zero_point=106\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.018633464351296425, zero_point=142, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.004596547689288855, zero_point=130, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.02003711834549904, zero_point=79\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.022134624421596527, zero_point=142, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.00513403071090579, zero_point=103, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.025713473558425903, zero_point=72\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.025206655263900757, zero_point=133, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.009287516586482525, zero_point=122, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), scale=0.007258214987814426, zero_point=116, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.017906757071614265, zero_point=112\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.011278004385530949, zero_point=131, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.00646337540820241, zero_point=134, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.02694826014339924, zero_point=118\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.013674738816916943, zero_point=160, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0027540342416614294, zero_point=98, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.08024650067090988, zero_point=86\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): QuantizedLinear(in_features=64, out_features=10, scale=0.1085096076130867, zero_point=50, qscheme=torch.per_tensor_affine)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet20_fp_model = CifarResNet(BasicBlock, [3]*3)\n",
        "\n",
        "resnet20_fp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet20/CIFAR-10/Train_iteration_3/Models/3rd_Iteration_retrained_model_quant_91.35acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet20_model = CifarResNet(BasicBlock, [3]*3)\n",
        "\n",
        "resnet20_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet20/CIFAR-10/Train_iteration_3/Models/3rd_Iteration_retrained_model_quant_91.35acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet20_model = torch.quantization.QuantWrapper(resnet20_model)\n",
        "B=8\n",
        "resnet20_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(resnet20_model, inplace=True)\n",
        "\n",
        "resnet20_model.to(\"cpu\")\n",
        "test(resnet20_model, testloader, cuda=False)\n",
        "resnet20_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(resnet20_model, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sbKrUEaYtaS",
        "outputId": "1d7c5d74-819b-40f9-f812-512d866445c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuantWrapper(\n",
              "  (quant): Quantize(scale=tensor([0.0203]), zero_point=tensor([120]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              "  (module): CifarResNet(\n",
              "    (conv1): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.03820133954286575, zero_point=125, padding=(1, 1), bias=False)\n",
              "    (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.012166980654001236, zero_point=161, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.006007997784763575, zero_point=136, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.021039722487330437, zero_point=67\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.015660101547837257, zero_point=140, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.004038095474243164, zero_point=134, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.01865542121231556, zero_point=55\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.014413267374038696, zero_point=147, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.0050149159505963326, zero_point=107, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.020246349275112152, zero_point=62\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.019287152215838432, zero_point=163, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.004275824874639511, zero_point=112, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.02103196457028389, zero_point=50\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.019647153094410896, zero_point=147, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.004552713129669428, zero_point=93, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.021354489028453827, zero_point=61\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (5): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.01828155480325222, zero_point=167, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.0023751521948724985, zero_point=134, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.020332451909780502, zero_point=37\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (6): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.023772655054926872, zero_point=159, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.004254778381437063, zero_point=160, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.02387622930109501, zero_point=67\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (7): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.037209074944257736, zero_point=119, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.005612807814031839, zero_point=95, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.022209331393241882, zero_point=51\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (8): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.020953916013240814, zero_point=145, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.003419045824557543, zero_point=124, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.021314408630132675, zero_point=34\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.03066367283463478, zero_point=133, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.007906423881649971, zero_point=125, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), scale=0.011831480078399181, zero_point=110, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.022564353421330452, zero_point=98\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.01340162381529808, zero_point=144, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.002251559169963002, zero_point=125, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.018321871757507324, zero_point=52\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.012888436205685139, zero_point=146, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0017164761666208506, zero_point=121, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.01865733042359352, zero_point=43\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.013169209472835064, zero_point=149, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0015234878519549966, zero_point=112, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.020696396008133888, zero_point=49\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.012868490070104599, zero_point=142, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0014297948218882084, zero_point=127, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.021967824548482895, zero_point=42\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (5): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.013974085450172424, zero_point=144, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.001699491636827588, zero_point=88, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.025735143572092056, zero_point=32\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (6): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.014720682054758072, zero_point=147, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.001764438464306295, zero_point=92, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.034161828458309174, zero_point=26\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (7): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.017171325162053108, zero_point=145, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0015873231459408998, zero_point=101, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.034186601638793945, zero_point=31\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (8): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.01896054670214653, zero_point=117, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.002598558785393834, zero_point=97, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.03770024701952934, zero_point=31\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.026644466444849968, zero_point=103, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.008019495755434036, zero_point=96, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), scale=0.010377614758908749, zero_point=80, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.020203933119773865, zero_point=86\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.011191710829734802, zero_point=128, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0018557237926870584, zero_point=137, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.017250047996640205, zero_point=61\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.010333297774195671, zero_point=137, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0017219875007867813, zero_point=130, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.017750956118106842, zero_point=58\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.009787295013666153, zero_point=126, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0020538289099931717, zero_point=124, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.01795583963394165, zero_point=62\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.009590376168489456, zero_point=128, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0018131386023014784, zero_point=118, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.019388370215892792, zero_point=66\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (5): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.009126696735620499, zero_point=124, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.002008266979828477, zero_point=108, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.023236943408846855, zero_point=58\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (6): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.01265235710889101, zero_point=128, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.003223992418497801, zero_point=105, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.031227940693497658, zero_point=74\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (7): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.01710752584040165, zero_point=123, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.004726084414869547, zero_point=94, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.05453458055853844, zero_point=72\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (8): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.028643913567066193, zero_point=194, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0015213274164125323, zero_point=80, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.055078133940696716, zero_point=41\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): QuantizedLinear(in_features=64, out_features=10, scale=0.07391655445098877, zero_point=51, qscheme=torch.per_tensor_affine)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet56_fp_model = CifarResNet(BasicBlock,[9]*3)\n",
        "\n",
        "resnet56_fp_model.load_state_dict(torch.load(\"/content/drive/Thesis/Thesis/ResNet56/CIFAR-10/Train Iteration 3/model/ResNet56_3it_CIFAR10_92.33acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet56_model = CifarResNet(BasicBlock,[9]*3)\n",
        "\n",
        "resnet56_model.load_state_dict(torch.load(\"/content/drive/Thesis/Thesis/ResNet56/CIFAR-10/Train Iteration 3/model/ResNet56_3it_CIFAR10_92.33acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet56_model = torch.quantization.QuantWrapper(resnet56_model)\n",
        "B=8\n",
        "resnet56_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(resnet56_model, inplace=True)\n",
        "\n",
        "resnet56_model.to(\"cpu\")\n",
        "test(resnet56_model, testloader, cuda=False)\n",
        "resnet56_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(resnet56_model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKqjZwZlb5ir",
        "outputId": "33730544-838d-4972-a540-b7663c80e503"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:317: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "QuantWrapper(\n",
              "  (quant): Quantize(scale=tensor([0.0203]), zero_point=tensor([120]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              "  (module): MobileNetV2(\n",
              "    (features): Sequential(\n",
              "      (0): ConvBNActivation(\n",
              "        (0): QuantizedConv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), scale=0.0380074605345726, zero_point=120, padding=(1, 1), bias=False)\n",
              "        (1): QuantizedBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): QuantizedReLU6(inplace=True)\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), scale=0.006269583944231272, zero_point=109, padding=(1, 1), groups=48, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): QuantizedConv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.005197020247578621, zero_point=134, bias=False)\n",
              "          (2): QuantizedBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.0059815882705152035, zero_point=97, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.0039605749770998955, zero_point=90, padding=(1, 1), groups=144, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.0061927977949380875, zero_point=145, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.004888174124062061, zero_point=148, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.0012632313882932067, zero_point=141, padding=(1, 1), groups=192, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.006013627164065838, zero_point=129, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.018046637997031212, zero_point=133\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (4): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.008622635155916214, zero_point=125, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), scale=0.0032486175186932087, zero_point=117, padding=(1, 1), groups=192, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.005767364054918289, zero_point=134, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (5): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.004748890642076731, zero_point=124, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), scale=0.0012063708854839206, zero_point=116, padding=(1, 1), groups=288, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.0036583933979272842, zero_point=146, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.015103377401828766, zero_point=116\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (6): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.006185594480484724, zero_point=129, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), scale=0.0010568476282060146, zero_point=130, padding=(1, 1), groups=288, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.0025649014860391617, zero_point=99, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.01879856176674366, zero_point=108\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (7): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.011992317624390125, zero_point=120, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), scale=0.00310823624022305, zero_point=115, padding=(1, 1), groups=288, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), scale=0.005363720469176769, zero_point=117, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (8): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), scale=0.004146145656704903, zero_point=118, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), scale=0.0007644861470907927, zero_point=146, padding=(1, 1), groups=528, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), scale=0.0015314316842705011, zero_point=121, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.011335576884448528, zero_point=111\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (9): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), scale=0.004656692501157522, zero_point=126, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), scale=0.0007324049365706742, zero_point=129, padding=(1, 1), groups=528, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), scale=0.001104985480196774, zero_point=125, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.012217813171446323, zero_point=115\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (10): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), scale=0.0053289299830794334, zero_point=126, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), scale=0.000710373162291944, zero_point=129, padding=(1, 1), groups=528, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), scale=0.0010940018109977245, zero_point=131, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.012946940958499908, zero_point=120\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (11): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), scale=0.010100713931024075, zero_point=99, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), scale=0.001883469638414681, zero_point=104, padding=(1, 1), groups=528, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), scale=0.004183450248092413, zero_point=134, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (12): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), scale=0.004456179682165384, zero_point=118, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), scale=0.0007502708467654884, zero_point=130, padding=(1, 1), groups=816, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), scale=0.0013292186195030808, zero_point=124, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.010633887723088264, zero_point=122\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (13): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), scale=0.005421902053058147, zero_point=113, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), scale=0.0009512596298009157, zero_point=131, padding=(1, 1), groups=816, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), scale=0.001243128441274166, zero_point=126, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.013251014985144138, zero_point=127\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (14): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), scale=0.009772446937859058, zero_point=106, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), scale=0.0020121452398598194, zero_point=88, padding=(1, 1), groups=816, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), scale=0.003336592810228467, zero_point=133, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (15): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), scale=0.0026806993409991264, zero_point=122, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), scale=0.0004782605974469334, zero_point=140, padding=(1, 1), groups=1344, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), scale=0.0008869307348504663, zero_point=151, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.008399903774261475, zero_point=140\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (16): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), scale=0.0032839912455528975, zero_point=116, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), scale=0.0006350848125293851, zero_point=106, padding=(1, 1), groups=1344, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), scale=0.0007926035323180258, zero_point=137, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.01063099130988121, zero_point=132\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (17): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), scale=0.005204902496188879, zero_point=117, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), scale=0.0006350512849166989, zero_point=156, padding=(1, 1), groups=1344, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), scale=0.00035547895822674036, zero_point=135, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (18): ConvBNActivation(\n",
              "        (0): QuantizedConv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), scale=0.000775583554059267, zero_point=103, bias=False)\n",
              "        (1): QuantizedBatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): QuantizedReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (classifier): Sequential(\n",
              "      (0): QuantizedDropout(p=0.2, inplace=False)\n",
              "      (1): QuantizedLinear(in_features=1792, out_features=10, scale=0.07040149718523026, zero_point=54, qscheme=torch.per_tensor_affine)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mobilenet_fp_model = MobileNetV2(width_mult = 1.4)\n",
        "\n",
        "mobilenet_fp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/MobileNetv2_x1_4/CIFAR-10/Train Iteration 3/model/MobileNet_3it_CIFAR10_92.79acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "mobilenet_model = MobileNetV2(width_mult = 1.4)\n",
        "\n",
        "mobilenet_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/MobileNetv2_x1_4/CIFAR-10/Train Iteration 3/model/MobileNet_3it_CIFAR10_92.79acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "mobilenet_model = torch.quantization.QuantWrapper(mobilenet_model)\n",
        "B=8\n",
        "mobilenet_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(mobilenet_model, inplace=True)\n",
        "\n",
        "mobilenet_model.to(\"cpu\")\n",
        "test(mobilenet_model, testloader, cuda=False)\n",
        "mobilenet_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(mobilenet_model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuh6OtM4eYHf"
      },
      "outputs": [],
      "source": [
        "# ResNet20 DataLoaders\n",
        "\n",
        "test_20_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_test_0)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_20_0_loader = torch.utils.data.DataLoader(dataset=test_20_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_test_1)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_20_1_loader = torch.utils.data.DataLoader(dataset=test_20_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_test_2)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_20_2_loader = torch.utils.data.DataLoader(dataset=test_20_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_test_3)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_20_3_loader = torch.utils.data.DataLoader(dataset=test_20_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_20_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_train_0)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_20_0_loader = torch.utils.data.DataLoader(dataset=train_20_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_train_1)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_20_1_loader = torch.utils.data.DataLoader(dataset=train_20_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_train_2)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_20_2_loader = torch.utils.data.DataLoader(dataset=train_20_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_train_3)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_20_3_loader = torch.utils.data.DataLoader(dataset=train_20_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPCX1BJ3g3oA"
      },
      "outputs": [],
      "source": [
        "# ResNet56 DataLoaders\n",
        "\n",
        "test_56_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_test_0)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_56_0_loader = torch.utils.data.DataLoader(dataset=test_56_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_test_1)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_56_1_loader = torch.utils.data.DataLoader(dataset=test_56_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_test_2)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_56_2_loader = torch.utils.data.DataLoader(dataset=test_56_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_test_3)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_56_3_loader = torch.utils.data.DataLoader(dataset=test_56_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_56_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_train_0)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_56_0_loader = torch.utils.data.DataLoader(dataset=train_56_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_train_1)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_56_1_loader = torch.utils.data.DataLoader(dataset=train_56_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_train_2)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_56_2_loader = torch.utils.data.DataLoader(dataset=train_56_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_train_3)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_56_3_loader = torch.utils.data.DataLoader(dataset=train_56_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5njPc3PthLbX"
      },
      "outputs": [],
      "source": [
        "#MobileNet Dataset\n",
        "\n",
        "test_mob_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_test_0)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_mob_0_loader = torch.utils.data.DataLoader(dataset=test_mob_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_test_1)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_mob_1_loader = torch.utils.data.DataLoader(dataset=test_mob_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_test_2)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_mob_2_loader = torch.utils.data.DataLoader(dataset=test_mob_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_test_3)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_mob_3_loader = torch.utils.data.DataLoader(dataset=test_mob_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_mob_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_train_0)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_mob_0_loader = torch.utils.data.DataLoader(dataset=train_mob_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_train_1)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_mob_1_loader = torch.utils.data.DataLoader(dataset=train_mob_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_train_2)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_mob_2_loader = torch.utils.data.DataLoader(dataset=train_mob_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_train_3)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_mob_3_loader = torch.utils.data.DataLoader(dataset=train_mob_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc4bfqRciapC",
        "outputId": "1296d908-8440-420e-b6c1-d27c1d95a9ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CIFAR-10 TEST SETS:\n",
            "ResNet20 0: Top_1: 90.75% Top_5:99.75%\n",
            "ResNet20 1: Top_1: 90.25% Top_5:100.00%\n",
            "ResNet20 2: Top_1: 88.00% Top_5:100.00%\n",
            "ResNet20 3: Top_1: 73.00% Top_5:99.75%\n",
            "\n",
            "\n",
            "ResNet56 0: Top_1: 92.50% Top_5:99.75%\n",
            "ResNet56 1: Top_1: 92.00% Top_5:99.75%\n",
            "ResNet56 2: Top_1: 91.25% Top_5:99.75%\n",
            "ResNet56 3: Top_1: 73.00% Top_5:99.75%\n",
            "\n",
            "\n",
            "MobileNet 0: Top_1: 92.25% Top_5:100.00%\n",
            "MobileNet 1: Top_1: 92.50% Top_5:100.00%\n",
            "MobileNet 2: Top_1: 89.75% Top_5:99.50%\n",
            "MobileNet 3: Top_1: 76.25% Top_5:99.25%\n"
          ]
        }
      ],
      "source": [
        "print(\"CIFAR-10 TEST SETS:\")\n",
        "\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_0_loader)\n",
        "print(\"ResNet20 0: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_1_loader)\n",
        "print(\"ResNet20 1: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_2_loader)\n",
        "print(\"ResNet20 2: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_3_loader)\n",
        "print(\"ResNet20 3: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_0_loader)\n",
        "print(\"ResNet56 0: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_1_loader)\n",
        "print(\"ResNet56 1: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_2_loader)\n",
        "print(\"ResNet56 2: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_3_loader)\n",
        "print(\"ResNet56 3: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_0_loader)\n",
        "print(\"MobileNet 0: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_1_loader)\n",
        "print(\"MobileNet 1: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_2_loader)\n",
        "print(\"MobileNet 2: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_3_loader)\n",
        "print(\"MobileNet 3: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZMTq7PY10WP",
        "outputId": "05c4cb5d-41cc-4ee1-bc92-08a757f283ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CIFAR-10 TRAIN SETS:\n",
            "ResNet20 0: Top_1: 100.00% Top_5:100.00%\n",
            "ResNet20 1: Top_1: 99.25% Top_5:100.00%\n",
            "ResNet20 2: Top_1: 99.50% Top_5:100.00%\n",
            "ResNet20 3: Top_1: 81.50% Top_5:100.00%\n",
            "\n",
            "\n",
            "ResNet56 0: Top_1: 100.00% Top_5:100.00%\n",
            "ResNet56 1: Top_1: 100.00% Top_5:100.00%\n",
            "ResNet56 2: Top_1: 98.50% Top_5:100.00%\n",
            "ResNet56 3: Top_1: 79.25% Top_5:100.00%\n",
            "\n",
            "\n",
            "MobileNet 0: Top_1: 100.00% Top_5:100.00%\n",
            "MobileNet 1: Top_1: 100.00% Top_5:100.00%\n",
            "MobileNet 2: Top_1: 99.25% Top_5:100.00%\n",
            "MobileNet 3: Top_1: 46.00% Top_5:75.25%\n"
          ]
        }
      ],
      "source": [
        "print(\"CIFAR-10 TRAIN SETS:\")\n",
        "\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_0_loader)\n",
        "print(\"ResNet20 0: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_1_loader)\n",
        "print(\"ResNet20 1: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_2_loader)\n",
        "print(\"ResNet20 2: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_3_loader)\n",
        "print(\"ResNet20 3: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_0_loader)\n",
        "print(\"ResNet56 0: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_1_loader)\n",
        "print(\"ResNet56 1: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_2_loader)\n",
        "print(\"ResNet56 2: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_3_loader)\n",
        "print(\"ResNet56 3: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_0_loader)\n",
        "print(\"MobileNet 0: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_1_loader)\n",
        "print(\"MobileNet 1: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_2_loader)\n",
        "print(\"MobileNet 2: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_3_loader)\n",
        "print(\"MobileNet 3: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlWihQcCPac5",
        "outputId": "4677f406-3d7e-465c-9a8f-e3294ce4b49a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CIFAR-10 TESTSET EVALUATOR\n",
            "ResNet20 Floating-Point Evaluation: Top 1: 91.35% Top 5: 99.56%\n",
            "ResNet20 Quantized Evaluation: Top 1: 91.28% Top 5: 99.58%\n",
            "ResNet56 Floating-Point Evaluation: Top 1: 92.33% Top 5: 99.67%\n",
            "ResNet56 Quantized Evaluation: Top 1: 92.24% Top 5: 99.62%\n",
            "MobileNet Floating-Point Evaluation: Top 1: 92.79% Top 5: 99.77%\n",
            "MobileNet Quantized Evaluation: Top 1: 92.37% Top 5: 99.77%\n"
          ]
        }
      ],
      "source": [
        "print(\"CIFAR-10 TESTSET EVALUATOR\")\n",
        "\n",
        "res20_top1, res20_top5 = evaluator_testset(resnet20_fp_model, testloader)\n",
        "print(\"ResNet20 Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(res20_top1, res20_top5))\n",
        "\n",
        "res20_top1, res20_top5 = evaluator_testset(resnet20_model, testloader)\n",
        "print(\"ResNet20 Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(res20_top1, res20_top5))\n",
        "\n",
        "\n",
        "res56_top1, res56_top5 = evaluator_testset(resnet56_fp_model, testloader)\n",
        "print(\"ResNet56 Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(res56_top1, res56_top5))\n",
        "\n",
        "res56_top1, res56_top5 = evaluator_testset(resnet56_model, testloader)\n",
        "print(\"ResNet56 Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(res56_top1, res56_top5))\n",
        "\n",
        "\n",
        "mob_top1, mob_top5 = evaluator_testset(mobilenet_fp_model, testloader)\n",
        "print(\"MobileNet Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(mob_top1, mob_top5))\n",
        "\n",
        "mob_top1, mob_top5 = evaluator_testset(mobilenet_model, testloader)\n",
        "print(\"MobileNet Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(mob_top1, mob_top5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1asX20jl7qiN"
      },
      "source": [
        "## FashionMNIST ResNet Quantized Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvDDOGkM7qiO"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from functools import partial\n",
        "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
        "from torch.ao.nn.quantized.modules.functional_modules import FloatFunctional\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.ff = torch.nn.quantized.FloatFunctional()\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        #out += identity\n",
        "        out = self.ff.add(out, identity)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(CifarResNet, self).__init__()\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = conv3x3(1, 16)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZtqRsUQ7qiP"
      },
      "source": [
        "## FashionMNIST MobileNet Quantized Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxYDqO6q7qiQ"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
        "from torch import Tensor\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from torch.ao.nn.quantized.modules.functional_modules import FloatFunctional\n",
        "\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        f_add = torch.nn.quantized.FloatFunctional()\n",
        "        new_v = f_add.add(new_v, divisor)\n",
        "        #new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class ConvBNActivation(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_planes: int,\n",
        "        out_planes: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        dilation: int = 1,\n",
        "    ) -> None:\n",
        "        padding = (kernel_size - 1) // 2 * dilation\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if activation_layer is None:\n",
        "            activation_layer = nn.ReLU6\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups,\n",
        "                      bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            activation_layer(inplace=True)\n",
        "        )\n",
        "        self.out_channels = out_planes\n",
        "\n",
        "\n",
        "# necessary for backwards compatibility\n",
        "ConvBNReLU = ConvBNActivation\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp: int,\n",
        "        oup: int,\n",
        "        stride: int,\n",
        "        expand_ratio: int,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "        if expand_ratio != 1:\n",
        "            # pw\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n",
        "        layers.extend([\n",
        "            # dw\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n",
        "            # pw-linear\n",
        "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "            norm_layer(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        self.out_channels = oup\n",
        "        self._is_cn = stride > 1\n",
        "        self.ff = torch.nn.quantized.FloatFunctional()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.use_res_connect:\n",
        "            #return x + self.conv(x)\n",
        "            return self.ff.add(x, self.conv(x))\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 10,\n",
        "        width_mult: float = 1.0,\n",
        "        inverted_residual_setting: Optional[List[List[int]]] = None,\n",
        "        round_nearest: int = 8,\n",
        "        block: Optional[Callable[..., nn.Module]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        MobileNet V2 main class\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes\n",
        "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
        "            inverted_residual_setting: Network structure\n",
        "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
        "            Set to 1 to turn off rounding\n",
        "            block: Module specifying inverted residual building block for mobilenet\n",
        "            norm_layer: Module specifying the normalization layer to use\n",
        "\n",
        "        \"\"\"\n",
        "        super(MobileNetV2, self).__init__()\n",
        "\n",
        "        if block is None:\n",
        "            block = InvertedResidual\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        if inverted_residual_setting is None:\n",
        "            inverted_residual_setting = [\n",
        "                # t, c, n, s\n",
        "                [1, 16, 1, 1],\n",
        "                [6, 24, 2, 1],  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
        "                [6, 32, 3, 2],\n",
        "                [6, 64, 4, 2],\n",
        "                [6, 96, 3, 1],\n",
        "                [6, 160, 3, 2],\n",
        "                [6, 320, 1, 1],\n",
        "            ]\n",
        "\n",
        "        # only check the first element, assuming user knows t,c,n,s are required\n",
        "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
        "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
        "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
        "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
        "        features: List[nn.Module] = [ConvBNReLU(1, input_channel, stride=1, norm_layer=norm_layer)]  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # This exists since TorchScript doesn't support inheritance, so the superclass method\n",
        "        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n",
        "        x = self.features(x)\n",
        "        # Cannot use \"squeeze\" as batch-size can be 1\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8UA9Cho7qiQ"
      },
      "source": [
        "## FashionMNIST Model Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiKrP25D7qiR",
        "outputId": "f157cd9f-d9b5-4efb-88aa-4456dfe84599"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuantWrapper(\n",
              "  (quant): Quantize(scale=tensor([0.0039]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              "  (module): CifarResNet(\n",
              "    (conv1): QuantizedConv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.010157379321753979, zero_point=130, padding=(1, 1), bias=False)\n",
              "    (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.021488258615136147, zero_point=178, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.014824401587247849, zero_point=134, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.02739982306957245, zero_point=93\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.029431454837322235, zero_point=157, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.009536550380289555, zero_point=130, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.026408841833472252, zero_point=86\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.029244093224406242, zero_point=123, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.008343602530658245, zero_point=123, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.02546723000705242, zero_point=68\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.028515387326478958, zero_point=130, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.01607818901538849, zero_point=88, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), scale=0.012716744095087051, zero_point=103, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.028316153213381767, zero_point=91\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.023027705028653145, zero_point=120, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.007657849695533514, zero_point=128, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.02590239606797695, zero_point=67\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.021466605365276337, zero_point=118, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.005539295729249716, zero_point=128, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.025052638724446297, zero_point=57\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.022669177502393723, zero_point=117, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.010665692389011383, zero_point=117, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), scale=0.007850684225559235, zero_point=123, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.01813085563480854, zero_point=119\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.011982334777712822, zero_point=129, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.004773726221174002, zero_point=126, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.021943239495158195, zero_point=102\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.013378553092479706, zero_point=164, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0018649556441232562, zero_point=88, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.03860762342810631, zero_point=72\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): QuantizedLinear(in_features=64, out_features=10, scale=0.0793156698346138, zero_point=62, qscheme=torch.per_tensor_affine)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet20_fp_model = CifarResNet(BasicBlock, [3]*3)\n",
        "\n",
        "resnet20_fp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet20/FashionMNIST/Train Iteration 3/model/ResNet20_3it_Fashion_93.01acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet20_model = CifarResNet(BasicBlock, [3]*3)\n",
        "\n",
        "resnet20_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet20/FashionMNIST/Train Iteration 3/model/ResNet20_3it_Fashion_93.01acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "resnet20_model = torch.quantization.QuantWrapper(resnet20_model)\n",
        "B=8\n",
        "resnet20_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(resnet20_model, inplace=True)\n",
        "\n",
        "resnet20_model.to(\"cpu\")\n",
        "test(resnet20_model, fashion_testloader, cuda=False)\n",
        "resnet20_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(resnet20_model, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw0jaFTS7qiS",
        "outputId": "116fd278-ec04-4bd0-af79-4b73ee6b8cee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuantWrapper(\n",
              "  (quant): Quantize(scale=tensor([0.0039]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              "  (module): CifarResNet(\n",
              "    (conv1): QuantizedConv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.006637860555201769, zero_point=118, padding=(1, 1), bias=False)\n",
              "    (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.006438401993364096, zero_point=156, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.00343819591216743, zero_point=127, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.014704804867506027, zero_point=71\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.008882750757038593, zero_point=125, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.002376198535785079, zero_point=154, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.01485526654869318, zero_point=60\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.0075843241065740585, zero_point=144, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.0022355488035827875, zero_point=123, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.015537632629275322, zero_point=51\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.007125122472643852, zero_point=156, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.001228034496307373, zero_point=129, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.015459530055522919, zero_point=46\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.008823233656585217, zero_point=135, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.001070417813025415, zero_point=160, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.01626693271100521, zero_point=49\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (5): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.00569633673876524, zero_point=93, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.001165758352726698, zero_point=148, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.015546212904155254, zero_point=33\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (6): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.008295045234262943, zero_point=120, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.0012308924924582243, zero_point=151, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.016595568507909775, zero_point=46\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (7): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.005595363676548004, zero_point=131, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.0007433028658851981, zero_point=149, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.015429941937327385, zero_point=28\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (8): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.0061251032166182995, zero_point=79, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.0009245914407074451, zero_point=99, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.015396569855511189, zero_point=25\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.008477439172565937, zero_point=136, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.002653019968420267, zero_point=118, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), scale=0.005103472154587507, zero_point=94, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.015542859211564064, zero_point=88\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.005228222347795963, zero_point=129, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.001425319816917181, zero_point=127, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.01359552051872015, zero_point=54\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.004756863694638014, zero_point=126, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.001177382655441761, zero_point=115, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.014121102169156075, zero_point=44\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.004479439929127693, zero_point=127, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0006980623584240675, zero_point=134, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.014001420699059963, zero_point=39\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.00522595876827836, zero_point=138, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0008666518842801452, zero_point=136, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.014357180334627628, zero_point=48\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (5): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.004172109067440033, zero_point=132, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0006312330369837582, zero_point=137, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.01396997831761837, zero_point=35\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (6): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.004607618786394596, zero_point=110, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.000748715887311846, zero_point=124, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.014313873834908009, zero_point=35\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (7): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.003873539390042424, zero_point=124, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0006161112687550485, zero_point=116, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.014177000150084496, zero_point=31\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (8): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.004018709529191256, zero_point=118, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0005731033161282539, zero_point=142, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.014512280933558941, zero_point=33\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.0067176613956689835, zero_point=117, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0021744901314377785, zero_point=118, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), scale=0.002952741226181388, zero_point=117, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.008143190294504166, zero_point=121\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0025548061821609735, zero_point=131, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.000976816052570939, zero_point=136, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.007512135896831751, zero_point=86\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0029266520868986845, zero_point=123, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0008422058308497071, zero_point=128, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.009734009392559528, zero_point=79\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.003012843430042267, zero_point=131, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0006415820098482072, zero_point=115, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.011402448639273643, zero_point=65\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.004650631919503212, zero_point=98, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0007993289618752897, zero_point=118, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.017656009644269943, zero_point=66\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (5): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.006702583748847246, zero_point=122, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0007009753026068211, zero_point=99, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.0198054239153862, zero_point=36\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (6): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.008305564522743225, zero_point=156, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0005201926687732339, zero_point=78, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.022289229556918144, zero_point=24\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (7): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.009609186090528965, zero_point=164, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.00046362532884813845, zero_point=70, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.024154312908649445, zero_point=16\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (8): BasicBlock(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.009210014715790749, zero_point=176, padding=(1, 1), bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.00045000400859862566, zero_point=76, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ff): QFunctional(\n",
              "          scale=0.026657231152057648, zero_point=15\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): QuantizedLinear(in_features=64, out_features=10, scale=0.04275244474411011, zero_point=56, qscheme=torch.per_tensor_affine)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet56_fp_model = CifarResNet(BasicBlock,[9]*3)\n",
        "\n",
        "resnet56_fp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet56/FashionMNIST/Train Iteration 3/model/ResNet56_3it_Fashion_93.29acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet56_model = CifarResNet(BasicBlock,[9]*3)\n",
        "\n",
        "resnet56_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet56/FashionMNIST/Train Iteration 3/model/ResNet56_3it_Fashion_93.29acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "resnet56_model = torch.quantization.QuantWrapper(resnet56_model)\n",
        "B=8\n",
        "resnet56_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(resnet56_model, inplace=True)\n",
        "\n",
        "resnet56_model.to(\"cpu\")\n",
        "test(resnet56_model, fashion_testloader, cuda=False)\n",
        "resnet56_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(resnet56_model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR1R8uNR7qiT",
        "outputId": "e3825ae3-51be-4a8f-a947-6731c1a480a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuantWrapper(\n",
              "  (quant): Quantize(scale=tensor([0.0039]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              "  (module): MobileNetV2(\n",
              "    (features): Sequential(\n",
              "      (0): ConvBNActivation(\n",
              "        (0): QuantizedConv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), scale=0.00594725925475359, zero_point=137, padding=(1, 1), bias=False)\n",
              "        (1): QuantizedBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): QuantizedReLU6(inplace=True)\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), scale=0.01379318442195654, zero_point=102, padding=(1, 1), groups=48, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): QuantizedConv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.050544675439596176, zero_point=122, bias=False)\n",
              "          (2): QuantizedBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.030932025983929634, zero_point=134, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.012904737144708633, zero_point=103, padding=(1, 1), groups=144, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.08781713992357254, zero_point=147, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.029483430087566376, zero_point=123, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.008577036671340466, zero_point=116, padding=(1, 1), groups=192, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.11310532689094543, zero_point=128, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.08583229035139084, zero_point=130\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (4): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.04716644063591957, zero_point=132, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), scale=0.011467684991657734, zero_point=106, padding=(1, 1), groups=192, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.09459707885980606, zero_point=134, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (5): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.03260137513279915, zero_point=130, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), scale=0.007318302523344755, zero_point=117, padding=(1, 1), groups=288, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.10610317438840866, zero_point=116, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.07732532173395157, zero_point=120\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (6): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.04183569177985191, zero_point=131, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), scale=0.006615381687879562, zero_point=123, padding=(1, 1), groups=288, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.09939835220575333, zero_point=126, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.09966424852609634, zero_point=115\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (7): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.05600402504205704, zero_point=126, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), scale=0.008178696036338806, zero_point=104, padding=(1, 1), groups=288, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), scale=0.07803001999855042, zero_point=137, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (8): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), scale=0.027240607887506485, zero_point=127, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), scale=0.005281418561935425, zero_point=122, padding=(1, 1), groups=528, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), scale=0.09048973768949509, zero_point=125, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.06721708923578262, zero_point=129\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (9): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), scale=0.03506997600197792, zero_point=121, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), scale=0.004640884231775999, zero_point=118, padding=(1, 1), groups=528, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), scale=0.0838027223944664, zero_point=124, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.08450311422348022, zero_point=129\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (10): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), scale=0.04277202859520912, zero_point=125, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), scale=0.004640226252377033, zero_point=108, padding=(1, 1), groups=528, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), scale=0.08450691401958466, zero_point=128, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.09988697618246078, zero_point=129\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (11): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), scale=0.05288106948137283, zero_point=129, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), scale=0.005482877604663372, zero_point=125, padding=(1, 1), groups=528, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), scale=0.0707787275314331, zero_point=129, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (12): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), scale=0.022627245634794235, zero_point=128, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), scale=0.0043370542116463184, zero_point=121, padding=(1, 1), groups=816, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), scale=0.08567661792039871, zero_point=137, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.06146709993481636, zero_point=129\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (13): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), scale=0.033826738595962524, zero_point=126, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), scale=0.003934256732463837, zero_point=128, padding=(1, 1), groups=816, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), scale=0.08877985179424286, zero_point=132, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.07653193920850754, zero_point=133\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (14): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), scale=0.04321368783712387, zero_point=130, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), scale=0.004071750212460756, zero_point=120, padding=(1, 1), groups=816, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), scale=0.06811296194791794, zero_point=125, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (15): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), scale=0.023955555632710457, zero_point=128, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), scale=0.002428041771054268, zero_point=125, padding=(1, 1), groups=1344, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), scale=0.07694065570831299, zero_point=130, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.05961548537015915, zero_point=127\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (16): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), scale=0.03286140784621239, zero_point=135, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), scale=0.002035088138654828, zero_point=129, padding=(1, 1), groups=1344, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), scale=0.07936422526836395, zero_point=126, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=0.07655053585767746, zero_point=130\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (17): InvertedResidual(\n",
              "        (conv): Sequential(\n",
              "          (0): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), scale=0.04002755880355835, zero_point=123, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (1): ConvBNActivation(\n",
              "            (0): QuantizedConv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), scale=0.002174986060708761, zero_point=127, padding=(1, 1), groups=1344, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): QuantizedReLU6(inplace=True)\n",
              "          )\n",
              "          (2): QuantizedConv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), scale=0.05782321095466614, zero_point=127, bias=False)\n",
              "          (3): QuantizedBatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (ff): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (18): ConvBNActivation(\n",
              "        (0): QuantizedConv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), scale=0.0281764455139637, zero_point=114, bias=False)\n",
              "        (1): QuantizedBatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): QuantizedReLU6(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (classifier): Sequential(\n",
              "      (0): QuantizedDropout(p=0.2, inplace=False)\n",
              "      (1): QuantizedLinear(in_features=1792, out_features=10, scale=0.14309494197368622, zero_point=73, qscheme=torch.per_tensor_affine)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mobilenet_fp_model = MobileNetV2(width_mult = 1.4)\n",
        "\n",
        "mobilenet_fp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/MobileNetv2_x1_4/FashionMNIST/Train Iteration 3/model/MobileNet_3it_Fashion_93.31acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "mobilenet_model = MobileNetV2(width_mult = 1.4)\n",
        "\n",
        "mobilenet_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/MobileNetv2_x1_4/FashionMNIST/Train Iteration 3/model/MobileNet_3it_Fashion_93.31acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "mobilenet_model = torch.quantization.QuantWrapper(mobilenet_model)\n",
        "B=8\n",
        "mobilenet_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(mobilenet_model, inplace=True)\n",
        "\n",
        "mobilenet_model.to(\"cpu\")\n",
        "test(mobilenet_model, fashion_testloader, cuda=False)\n",
        "mobilenet_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(mobilenet_model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7I6i8HZ3bxR"
      },
      "outputs": [],
      "source": [
        "#ResNet20 DataLoaders\n",
        "\n",
        "test_20_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_test_0).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_20_0_loader = torch.utils.data.DataLoader(dataset=test_20_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_test_1).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_20_1_loader = torch.utils.data.DataLoader(dataset=test_20_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_test_2).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_20_2_loader = torch.utils.data.DataLoader(dataset=test_20_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_test_3).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_20_3_loader = torch.utils.data.DataLoader(dataset=test_20_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_20_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_train_0).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_20_0_loader = torch.utils.data.DataLoader(dataset=train_20_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_train_1).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_20_1_loader = torch.utils.data.DataLoader(dataset=train_20_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_train_2).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_20_2_loader = torch.utils.data.DataLoader(dataset=train_20_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_train_3).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_20_3_loader = torch.utils.data.DataLoader(dataset=train_20_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twU4O4UB3bxR"
      },
      "outputs": [],
      "source": [
        "#ResNet56 DataLoaders\n",
        "\n",
        "test_56_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_test_0).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_56_0_loader = torch.utils.data.DataLoader(dataset=test_56_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_test_1).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_56_1_loader = torch.utils.data.DataLoader(dataset=test_56_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_test_2).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_56_2_loader = torch.utils.data.DataLoader(dataset=test_56_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_test_3).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_56_3_loader = torch.utils.data.DataLoader(dataset=test_56_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_56_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_train_0).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_56_0_loader = torch.utils.data.DataLoader(dataset=train_56_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_train_1).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_56_1_loader = torch.utils.data.DataLoader(dataset=train_56_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_train_2).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_56_2_loader = torch.utils.data.DataLoader(dataset=train_56_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_train_3).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_56_3_loader = torch.utils.data.DataLoader(dataset=train_56_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TvUrJqY3bxS"
      },
      "outputs": [],
      "source": [
        "#MobileNet Dataset\n",
        "\n",
        "test_mob_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_test_0).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_mob_0_loader = torch.utils.data.DataLoader(dataset=test_mob_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_test_1).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_mob_1_loader = torch.utils.data.DataLoader(dataset=test_mob_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_test_2).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_mob_2_loader = torch.utils.data.DataLoader(dataset=test_mob_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_test_3).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_mob_3_loader = torch.utils.data.DataLoader(dataset=test_mob_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_mob_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_train_0).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_mob_0_loader = torch.utils.data.DataLoader(dataset=train_mob_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_train_1).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_mob_1_loader = torch.utils.data.DataLoader(dataset=train_mob_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_train_2).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_mob_2_loader = torch.utils.data.DataLoader(dataset=train_mob_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_train_3).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_mob_3_loader = torch.utils.data.DataLoader(dataset=train_mob_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDmaufNG3bxT",
        "outputId": "9ffb6af4-d75c-4ca4-eb1d-47c6fb071f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FashionMNIST TEST SETS:\n",
            "ResNet20 0: Top_1: 92.50% Top_5:99.75%\n",
            "ResNet20 1: Top_1: 93.00% Top_5:99.75%\n",
            "ResNet20 2: Top_1: 92.00% Top_5:99.75%\n",
            "ResNet20 3: Top_1: 5.50% Top_5:99.75%\n",
            "\n",
            "\n",
            "ResNet56 0: Top_1: 94.75% Top_5:99.75%\n",
            "ResNet56 1: Top_1: 94.25% Top_5:99.75%\n",
            "ResNet56 2: Top_1: 93.75% Top_5:99.75%\n",
            "ResNet56 3: Top_1: 4.00% Top_5:100.00%\n",
            "\n",
            "\n",
            "MobileNet 0: Top_1: 93.00% Top_5:99.25%\n",
            "MobileNet 1: Top_1: 92.75% Top_5:99.75%\n",
            "MobileNet 2: Top_1: 93.75% Top_5:99.50%\n",
            "MobileNet 3: Top_1: 3.75% Top_5:99.75%\n"
          ]
        }
      ],
      "source": [
        "print(\"FashionMNIST TEST SETS:\")\n",
        "\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_0_loader)\n",
        "print(\"ResNet20 0: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_1_loader)\n",
        "print(\"ResNet20 1: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_2_loader)\n",
        "print(\"ResNet20 2: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_3_loader)\n",
        "print(\"ResNet20 3: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_0_loader)\n",
        "print(\"ResNet56 0: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_1_loader)\n",
        "print(\"ResNet56 1: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_2_loader)\n",
        "print(\"ResNet56 2: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_3_loader)\n",
        "print(\"ResNet56 3: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_0_loader)\n",
        "print(\"MobileNet 0: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_1_loader)\n",
        "print(\"MobileNet 1: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_2_loader)\n",
        "print(\"MobileNet 2: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_3_loader)\n",
        "print(\"MobileNet 3: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUbg9ExN3bxU",
        "outputId": "8c579877-cdf4-480d-d5e0-6049a8c6e9ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FashionMNIST TRAIN SETS:\n",
            "ResNet20 0: Top_1: 99.00% Top_5:100.00%\n",
            "ResNet20 1: Top_1: 99.00% Top_5:100.00%\n",
            "ResNet20 2: Top_1: 99.75% Top_5:100.00%\n",
            "ResNet20 3: Top_1: 0.75% Top_5:100.00%\n",
            "\n",
            "\n",
            "ResNet56 0: Top_1: 99.75% Top_5:100.00%\n",
            "ResNet56 1: Top_1: 99.50% Top_5:100.00%\n",
            "ResNet56 2: Top_1: 99.75% Top_5:100.00%\n",
            "ResNet56 3: Top_1: 0.25% Top_5:100.00%\n",
            "\n",
            "\n",
            "MobileNet 0: Top_1: 99.75% Top_5:100.00%\n",
            "MobileNet 1: Top_1: 99.75% Top_5:100.00%\n",
            "MobileNet 2: Top_1: 99.75% Top_5:100.00%\n",
            "MobileNet 3: Top_1: 0.00% Top_5:100.00%\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "print(\"FashionMNIST TRAIN SETS:\")\n",
        "\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_0_loader)\n",
        "print(\"ResNet20 0: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_1_loader)\n",
        "print(\"ResNet20 1: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_2_loader)\n",
        "print(\"ResNet20 2: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_3_loader)\n",
        "print(\"ResNet20 3: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_0_loader)\n",
        "print(\"ResNet56 0: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_1_loader)\n",
        "print(\"ResNet56 1: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_2_loader)\n",
        "print(\"ResNet56 2: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_3_loader)\n",
        "print(\"ResNet56 3: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_0_loader)\n",
        "print(\"MobileNet 0: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_1_loader)\n",
        "print(\"MobileNet 1: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_2_loader)\n",
        "print(\"MobileNet 2: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_3_loader)\n",
        "print(\"MobileNet 3: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "print(test(mobilenet_model,train_mob_3_loader, cuda=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If1k4ASuRgtU",
        "outputId": "cc63a22f-b27b-4081-be80-85687c3d0a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FashionMNIST TESTSET EVALUATOR\n",
            "ResNet20 Floating-Point Evaluation: Top 1: 93.01% Top 5: 99.68%\n",
            "ResNet20 Quantized Evaluation: Top 1: 92.97% Top 5: 99.66%\n",
            "ResNet56 Floating-Point Evaluation: Top 1: 93.29% Top 5: 99.75%\n",
            "ResNet56 Quantized Evaluation: Top 1: 93.12% Top 5: 99.76%\n",
            "MobileNet Floating-Point Evaluation: Top 1: 93.31% Top 5: 99.44%\n",
            "MobileNet Quantized Evaluation: Top 1: 93.15% Top 5: 99.45%\n"
          ]
        }
      ],
      "source": [
        "print(\"FashionMNIST TESTSET EVALUATOR\")\n",
        "\n",
        "res20_top1, res20_top5 = evaluator_testset(resnet20_fp_model, fashion_testloader)\n",
        "print(\"ResNet20 Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(res20_top1, res20_top5))\n",
        "\n",
        "res20_top1, res20_top5 = evaluator_testset(resnet20_model, fashion_testloader)\n",
        "print(\"ResNet20 Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(res20_top1, res20_top5))\n",
        "\n",
        "\n",
        "res56_top1, res56_top5 = evaluator_testset(resnet56_fp_model, fashion_testloader)\n",
        "print(\"ResNet56 Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(res56_top1, res56_top5))\n",
        "\n",
        "res56_top1, res56_top5 = evaluator_testset(resnet56_model, fashion_testloader)\n",
        "print(\"ResNet56 Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(res56_top1, res56_top5))\n",
        "\n",
        "\n",
        "mob_top1, mob_top5 = evaluator_testset(mobilenet_fp_model, fashion_testloader)\n",
        "print(\"MobileNet Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(mob_top1, mob_top5))\n",
        "\n",
        "mob_top1, mob_top5 = evaluator_testset(mobilenet_model, fashion_testloader)\n",
        "print(\"MobileNet Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(mob_top1, mob_top5))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "O7TH2ujaUs_A",
        "_S3sPpkgUwmI",
        "zTh24FhMYC28",
        "KRSPVYrZa-n6",
        "vGBDJRVEccJA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
