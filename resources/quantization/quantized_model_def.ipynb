{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcclHiU5S94D",
        "outputId": "5db4b82d-7682-4bf5-d6d2-5e9d4cda2ef8"
      },
      "outputs": [],
      "source": [
        "!pip install adversarial-robustness-toolbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BAT_Np0S83v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import time\n",
        "from math import log10, sqrt\n",
        "from torch.utils.data import DataLoader\n",
        "from art.utils import load_cifar10\n",
        "import random\n",
        "from torch.quantization import MovingAverageMinMaxObserver\n",
        "from torch.ao.quantization.observer import MinMaxObserver\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQLrFqWZcmDY",
        "outputId": "13d6ce68-1f4b-4d17-e156-46000f3469ba"
      },
      "outputs": [],
      "source": [
        "def test(model: nn.Module, dataloader: DataLoader, cuda=False) -> float:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def predict(img):\n",
        "  img = torch.from_numpy(img)\n",
        "  img = F.normalize(img, [0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
        "  return img.numpy()\n",
        "\n",
        "def custom_collate(batch):\n",
        "        # Combine a list of samples into a batch\n",
        "        data, labels = zip(*batch)\n",
        "        data = torch.stack(data)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "        return data, labels\n",
        "\n",
        "def evaluator(model, loader):\n",
        "  model.eval()\n",
        "  top_1 = 0\n",
        "  top_5 = 0\n",
        "  with torch.no_grad():\n",
        "    for data in loader:\n",
        "      inputs, labels = data\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      _, predicted = torch.max(outputs, 1, keepdim=True)\n",
        "      top_1 += torch.sum(predicted.view(-1) == labels).item()\n",
        "\n",
        "      _, predicted_5 = torch.topk(outputs, k=5, dim=1)\n",
        "      top_5 += torch.sum(predicted_5 == labels.unsqueeze(1)).item()\n",
        "\n",
        "  return (\"{:.2f}\".format((top_1/400) * 100), \"{:.2f}\".format((top_5/400) * 100))\n",
        "\n",
        "def evaluator_testset(model, loader):\n",
        "  model.eval()\n",
        "  top_1 = 0\n",
        "  top_5 = 0\n",
        "  with torch.no_grad():\n",
        "    for data in loader:\n",
        "      inputs, labels = data\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      _, predicted = torch.max(outputs, 1, keepdim=True)\n",
        "      top_1 += torch.sum(predicted.view(-1) == labels).item()\n",
        "\n",
        "      _, predicted_5 = torch.topk(outputs, k=5, dim=1)\n",
        "      top_5 += torch.sum(predicted_5 == labels.unsqueeze(1)).item()\n",
        "\n",
        "  return (\"{:.2f}\".format((top_1/10000) * 100), \"{:.2f}\".format((top_5/10000) * 100))\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Thesis/FashionMnist_set/testset.pkl\", 'rb') as f:\n",
        "  fashion_testset = pickle.load(f)\n",
        "\n",
        "fashion_testloader = torch.utils.data.DataLoader(fashion_testset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIZa4x6LQj_E"
      },
      "outputs": [],
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = load_cifar10()\n",
        "\n",
        "x_train = np.transpose(x_train, (0, 3, 1, 2)).astype(np.float32)\n",
        "x_test = np.transpose(x_test, (0, 3, 1, 2)).astype(np.float32)\n",
        "\n",
        "classes_cifar = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "classes_fashion = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle-boot']\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Thesis/FashionMnist_set/fashionmnist_label.pkl\",'rb') as f:\n",
        "  y_f_train_set = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Thesis/FashionMnist_set/fashionmnist_test_label.pkl\",'rb') as f:\n",
        "  y_f_test_set = pickle.load(f)\n",
        "\n",
        "y_f_train_set = y_f_train_set[0:400]\n",
        "\n",
        "y_f_test_set = y_f_test_set[0:400]\n",
        "\n",
        "y_test_set = np.zeros((400,),np.int8)\n",
        "\n",
        "y_train_set = np.zeros((400,),np.int8)\n",
        "\n",
        "\n",
        "for i in range(400):\n",
        "        y_test_set[i] = np.where(y_test[i] == 1)[0][0]\n",
        "\n",
        "for i in range(400):\n",
        "        y_train_set[i] = np.where(y_train[i] == 1)[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTh24FhMYC28"
      },
      "source": [
        "## CIFAR-10 ResNet Quantized Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IL8voP3tYBCi"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from functools import partial\n",
        "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
        "from torch.ao.nn.quantized.modules.functional_modules import FloatFunctional\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.ff = torch.nn.quantized.FloatFunctional()\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        #out += identity\n",
        "        out = self.ff.add(out, identity)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(CifarResNet, self).__init__()\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = conv3x3(3, 16)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRSPVYrZa-n6"
      },
      "source": [
        "## CIFAR-10 MobileNet Quantized Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTyn6pfmbRTW"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
        "from torch import Tensor\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from torch.ao.nn.quantized.modules.functional_modules import FloatFunctional\n",
        "\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        f_add = torch.nn.quantized.FloatFunctional()\n",
        "        new_v = f_add.add(new_v, divisor)\n",
        "        #new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class ConvBNActivation(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_planes: int,\n",
        "        out_planes: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        dilation: int = 1,\n",
        "    ) -> None:\n",
        "        padding = (kernel_size - 1) // 2 * dilation\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if activation_layer is None:\n",
        "            activation_layer = nn.ReLU6\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups,\n",
        "                      bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            activation_layer(inplace=True)\n",
        "        )\n",
        "        self.out_channels = out_planes\n",
        "\n",
        "\n",
        "# necessary for backwards compatibility\n",
        "ConvBNReLU = ConvBNActivation\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp: int,\n",
        "        oup: int,\n",
        "        stride: int,\n",
        "        expand_ratio: int,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "        if expand_ratio != 1:\n",
        "            # pw\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n",
        "        layers.extend([\n",
        "            # dw\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n",
        "            # pw-linear\n",
        "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "            norm_layer(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        self.out_channels = oup\n",
        "        self._is_cn = stride > 1\n",
        "        self.ff = torch.nn.quantized.FloatFunctional()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.use_res_connect:\n",
        "            #return x + self.conv(x)\n",
        "            return self.ff.add(x, self.conv(x))\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 10,\n",
        "        width_mult: float = 1.0,\n",
        "        inverted_residual_setting: Optional[List[List[int]]] = None,\n",
        "        round_nearest: int = 8,\n",
        "        block: Optional[Callable[..., nn.Module]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        MobileNet V2 main class\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes\n",
        "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
        "            inverted_residual_setting: Network structure\n",
        "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
        "            Set to 1 to turn off rounding\n",
        "            block: Module specifying inverted residual building block for mobilenet\n",
        "            norm_layer: Module specifying the normalization layer to use\n",
        "\n",
        "        \"\"\"\n",
        "        super(MobileNetV2, self).__init__()\n",
        "\n",
        "        if block is None:\n",
        "            block = InvertedResidual\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        if inverted_residual_setting is None:\n",
        "            inverted_residual_setting = [\n",
        "                # t, c, n, s\n",
        "                [1, 16, 1, 1],\n",
        "                [6, 24, 2, 1],  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
        "                [6, 32, 3, 2],\n",
        "                [6, 64, 4, 2],\n",
        "                [6, 96, 3, 1],\n",
        "                [6, 160, 3, 2],\n",
        "                [6, 320, 1, 1],\n",
        "            ]\n",
        "\n",
        "        # only check the first element, assuming user knows t,c,n,s are required\n",
        "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
        "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
        "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
        "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
        "        features: List[nn.Module] = [ConvBNReLU(3, input_channel, stride=1, norm_layer=norm_layer)]  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # This exists since TorchScript doesn't support inheritance, so the superclass method\n",
        "        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n",
        "        x = self.features(x)\n",
        "        # Cannot use \"squeeze\" as batch-size can be 1\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGBDJRVEccJA"
      },
      "source": [
        "## Model Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_AOtzk0dkIU",
        "outputId": "e9cb126b-fdfc-40fc-d243-4c21e7a2cca6"
      },
      "outputs": [],
      "source": [
        "resnet20_fp_model = CifarResNet(BasicBlock, [3]*3)\n",
        "\n",
        "resnet20_fp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet20/CIFAR-10/Train_iteration_3/Models/3rd_Iteration_retrained_model_quant_91.35acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet20_model = CifarResNet(BasicBlock, [3]*3)\n",
        "\n",
        "resnet20_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet20/CIFAR-10/Train_iteration_3/Models/3rd_Iteration_retrained_model_quant_91.35acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet20_model = torch.quantization.QuantWrapper(resnet20_model)\n",
        "B=8\n",
        "resnet20_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(resnet20_model, inplace=True)\n",
        "\n",
        "resnet20_model.to(\"cpu\")\n",
        "test(resnet20_model, testloader, cuda=False)\n",
        "resnet20_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(resnet20_model, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sbKrUEaYtaS",
        "outputId": "1d7c5d74-819b-40f9-f812-512d866445c3"
      },
      "outputs": [],
      "source": [
        "resnet56_fp_model = CifarResNet(BasicBlock,[9]*3)\n",
        "\n",
        "resnet56_fp_model.load_state_dict(torch.load(\"/content/drive/Thesis/Thesis/ResNet56/CIFAR-10/Train Iteration 3/model/ResNet56_3it_CIFAR10_92.33acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet56_model = CifarResNet(BasicBlock,[9]*3)\n",
        "\n",
        "resnet56_model.load_state_dict(torch.load(\"/content/drive/Thesis/Thesis/ResNet56/CIFAR-10/Train Iteration 3/model/ResNet56_3it_CIFAR10_92.33acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet56_model = torch.quantization.QuantWrapper(resnet56_model)\n",
        "B=8\n",
        "resnet56_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(resnet56_model, inplace=True)\n",
        "\n",
        "resnet56_model.to(\"cpu\")\n",
        "test(resnet56_model, testloader, cuda=False)\n",
        "resnet56_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(resnet56_model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKqjZwZlb5ir",
        "outputId": "33730544-838d-4972-a540-b7663c80e503"
      },
      "outputs": [],
      "source": [
        "mobilenet_fp_model = MobileNetV2(width_mult = 1.4)\n",
        "\n",
        "mobilenet_fp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/MobileNetv2_x1_4/CIFAR-10/Train Iteration 3/model/MobileNet_3it_CIFAR10_92.79acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "mobilenet_model = MobileNetV2(width_mult = 1.4)\n",
        "\n",
        "mobilenet_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/MobileNetv2_x1_4/CIFAR-10/Train Iteration 3/model/MobileNet_3it_CIFAR10_92.79acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "mobilenet_model = torch.quantization.QuantWrapper(mobilenet_model)\n",
        "B=8\n",
        "mobilenet_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(mobilenet_model, inplace=True)\n",
        "\n",
        "mobilenet_model.to(\"cpu\")\n",
        "test(mobilenet_model, testloader, cuda=False)\n",
        "mobilenet_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(mobilenet_model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuh6OtM4eYHf"
      },
      "outputs": [],
      "source": [
        "# ResNet20 DataLoaders\n",
        "\n",
        "test_20_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_test_0)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_20_0_loader = torch.utils.data.DataLoader(dataset=test_20_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_test_1)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_20_1_loader = torch.utils.data.DataLoader(dataset=test_20_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_test_2)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_20_2_loader = torch.utils.data.DataLoader(dataset=test_20_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_test_3)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_20_3_loader = torch.utils.data.DataLoader(dataset=test_20_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_20_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_train_0)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_20_0_loader = torch.utils.data.DataLoader(dataset=train_20_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_train_1)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_20_1_loader = torch.utils.data.DataLoader(dataset=train_20_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_train_2)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_20_2_loader = torch.utils.data.DataLoader(dataset=train_20_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_20_train_3)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_20_3_loader = torch.utils.data.DataLoader(dataset=train_20_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPCX1BJ3g3oA"
      },
      "outputs": [],
      "source": [
        "# ResNet56 DataLoaders\n",
        "\n",
        "test_56_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_test_0)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_56_0_loader = torch.utils.data.DataLoader(dataset=test_56_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_test_1)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_56_1_loader = torch.utils.data.DataLoader(dataset=test_56_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_test_2)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_56_2_loader = torch.utils.data.DataLoader(dataset=test_56_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_test_3)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_56_3_loader = torch.utils.data.DataLoader(dataset=test_56_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_56_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_train_0)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_56_0_loader = torch.utils.data.DataLoader(dataset=train_56_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_train_1)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_56_1_loader = torch.utils.data.DataLoader(dataset=train_56_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_train_2)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_56_2_loader = torch.utils.data.DataLoader(dataset=train_56_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_56_train_3)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_56_3_loader = torch.utils.data.DataLoader(dataset=train_56_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5njPc3PthLbX"
      },
      "outputs": [],
      "source": [
        "#MobileNet Dataset\n",
        "\n",
        "test_mob_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_test_0)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_mob_0_loader = torch.utils.data.DataLoader(dataset=test_mob_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_test_1)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_mob_1_loader = torch.utils.data.DataLoader(dataset=test_mob_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_test_2)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_mob_2_loader = torch.utils.data.DataLoader(dataset=test_mob_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_test_3)).type(torch.FloatTensor), torch.from_numpy(y_test_set).type(torch.LongTensor))\n",
        "test_mob_3_loader = torch.utils.data.DataLoader(dataset=test_mob_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_mob_0 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_train_0)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_mob_0_loader = torch.utils.data.DataLoader(dataset=train_mob_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_1 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_train_1)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_mob_1_loader = torch.utils.data.DataLoader(dataset=train_mob_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_2 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_train_2)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_mob_2_loader = torch.utils.data.DataLoader(dataset=train_mob_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_3 = torch.utils.data.TensorDataset(torch.from_numpy(predict(x_mob_train_3)).type(torch.FloatTensor), torch.from_numpy(y_train_set).type(torch.LongTensor))\n",
        "train_mob_3_loader = torch.utils.data.DataLoader(dataset=train_mob_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc4bfqRciapC",
        "outputId": "1296d908-8440-420e-b6c1-d27c1d95a9ff"
      },
      "outputs": [],
      "source": [
        "print(\"CIFAR-10 TEST SETS:\")\n",
        "\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_0_loader)\n",
        "print(\"ResNet20 0: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_1_loader)\n",
        "print(\"ResNet20 1: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_2_loader)\n",
        "print(\"ResNet20 2: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_3_loader)\n",
        "print(\"ResNet20 3: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_0_loader)\n",
        "print(\"ResNet56 0: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_1_loader)\n",
        "print(\"ResNet56 1: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_2_loader)\n",
        "print(\"ResNet56 2: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_3_loader)\n",
        "print(\"ResNet56 3: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_0_loader)\n",
        "print(\"MobileNet 0: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_1_loader)\n",
        "print(\"MobileNet 1: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_2_loader)\n",
        "print(\"MobileNet 2: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_3_loader)\n",
        "print(\"MobileNet 3: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZMTq7PY10WP",
        "outputId": "05c4cb5d-41cc-4ee1-bc92-08a757f283ff"
      },
      "outputs": [],
      "source": [
        "print(\"CIFAR-10 TRAIN SETS:\")\n",
        "\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_0_loader)\n",
        "print(\"ResNet20 0: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_1_loader)\n",
        "print(\"ResNet20 1: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_2_loader)\n",
        "print(\"ResNet20 2: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_3_loader)\n",
        "print(\"ResNet20 3: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_0_loader)\n",
        "print(\"ResNet56 0: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_1_loader)\n",
        "print(\"ResNet56 1: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_2_loader)\n",
        "print(\"ResNet56 2: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_3_loader)\n",
        "print(\"ResNet56 3: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_0_loader)\n",
        "print(\"MobileNet 0: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_1_loader)\n",
        "print(\"MobileNet 1: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_2_loader)\n",
        "print(\"MobileNet 2: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_3_loader)\n",
        "print(\"MobileNet 3: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlWihQcCPac5",
        "outputId": "4677f406-3d7e-465c-9a8f-e3294ce4b49a"
      },
      "outputs": [],
      "source": [
        "print(\"CIFAR-10 TESTSET EVALUATOR\")\n",
        "\n",
        "res20_top1, res20_top5 = evaluator_testset(resnet20_fp_model, testloader)\n",
        "print(\"ResNet20 Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(res20_top1, res20_top5))\n",
        "\n",
        "res20_top1, res20_top5 = evaluator_testset(resnet20_model, testloader)\n",
        "print(\"ResNet20 Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(res20_top1, res20_top5))\n",
        "\n",
        "\n",
        "res56_top1, res56_top5 = evaluator_testset(resnet56_fp_model, testloader)\n",
        "print(\"ResNet56 Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(res56_top1, res56_top5))\n",
        "\n",
        "res56_top1, res56_top5 = evaluator_testset(resnet56_model, testloader)\n",
        "print(\"ResNet56 Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(res56_top1, res56_top5))\n",
        "\n",
        "\n",
        "mob_top1, mob_top5 = evaluator_testset(mobilenet_fp_model, testloader)\n",
        "print(\"MobileNet Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(mob_top1, mob_top5))\n",
        "\n",
        "mob_top1, mob_top5 = evaluator_testset(mobilenet_model, testloader)\n",
        "print(\"MobileNet Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(mob_top1, mob_top5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1asX20jl7qiN"
      },
      "source": [
        "## FashionMNIST ResNet Quantized Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvDDOGkM7qiO"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from functools import partial\n",
        "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
        "from torch.ao.nn.quantized.modules.functional_modules import FloatFunctional\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.ff = torch.nn.quantized.FloatFunctional()\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        #out += identity\n",
        "        out = self.ff.add(out, identity)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(CifarResNet, self).__init__()\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = conv3x3(1, 16)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZtqRsUQ7qiP"
      },
      "source": [
        "## FashionMNIST MobileNet Quantized Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxYDqO6q7qiQ"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
        "from torch import Tensor\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from torch.ao.nn.quantized.modules.functional_modules import FloatFunctional\n",
        "\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        f_add = torch.nn.quantized.FloatFunctional()\n",
        "        new_v = f_add.add(new_v, divisor)\n",
        "        #new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class ConvBNActivation(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_planes: int,\n",
        "        out_planes: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        dilation: int = 1,\n",
        "    ) -> None:\n",
        "        padding = (kernel_size - 1) // 2 * dilation\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if activation_layer is None:\n",
        "            activation_layer = nn.ReLU6\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups,\n",
        "                      bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            activation_layer(inplace=True)\n",
        "        )\n",
        "        self.out_channels = out_planes\n",
        "\n",
        "\n",
        "# necessary for backwards compatibility\n",
        "ConvBNReLU = ConvBNActivation\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp: int,\n",
        "        oup: int,\n",
        "        stride: int,\n",
        "        expand_ratio: int,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "        if expand_ratio != 1:\n",
        "            # pw\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n",
        "        layers.extend([\n",
        "            # dw\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n",
        "            # pw-linear\n",
        "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "            norm_layer(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        self.out_channels = oup\n",
        "        self._is_cn = stride > 1\n",
        "        self.ff = torch.nn.quantized.FloatFunctional()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.use_res_connect:\n",
        "            #return x + self.conv(x)\n",
        "            return self.ff.add(x, self.conv(x))\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 10,\n",
        "        width_mult: float = 1.0,\n",
        "        inverted_residual_setting: Optional[List[List[int]]] = None,\n",
        "        round_nearest: int = 8,\n",
        "        block: Optional[Callable[..., nn.Module]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        MobileNet V2 main class\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes\n",
        "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
        "            inverted_residual_setting: Network structure\n",
        "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
        "            Set to 1 to turn off rounding\n",
        "            block: Module specifying inverted residual building block for mobilenet\n",
        "            norm_layer: Module specifying the normalization layer to use\n",
        "\n",
        "        \"\"\"\n",
        "        super(MobileNetV2, self).__init__()\n",
        "\n",
        "        if block is None:\n",
        "            block = InvertedResidual\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        if inverted_residual_setting is None:\n",
        "            inverted_residual_setting = [\n",
        "                # t, c, n, s\n",
        "                [1, 16, 1, 1],\n",
        "                [6, 24, 2, 1],  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
        "                [6, 32, 3, 2],\n",
        "                [6, 64, 4, 2],\n",
        "                [6, 96, 3, 1],\n",
        "                [6, 160, 3, 2],\n",
        "                [6, 320, 1, 1],\n",
        "            ]\n",
        "\n",
        "        # only check the first element, assuming user knows t,c,n,s are required\n",
        "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
        "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
        "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
        "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
        "        features: List[nn.Module] = [ConvBNReLU(1, input_channel, stride=1, norm_layer=norm_layer)]  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # This exists since TorchScript doesn't support inheritance, so the superclass method\n",
        "        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n",
        "        x = self.features(x)\n",
        "        # Cannot use \"squeeze\" as batch-size can be 1\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8UA9Cho7qiQ"
      },
      "source": [
        "## FashionMNIST Model Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiKrP25D7qiR",
        "outputId": "f157cd9f-d9b5-4efb-88aa-4456dfe84599"
      },
      "outputs": [],
      "source": [
        "resnet20_fp_model = CifarResNet(BasicBlock, [3]*3)\n",
        "\n",
        "resnet20_fp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet20/FashionMNIST/Train Iteration 3/model/ResNet20_3it_Fashion_93.01acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet20_model = CifarResNet(BasicBlock, [3]*3)\n",
        "\n",
        "resnet20_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet20/FashionMNIST/Train Iteration 3/model/ResNet20_3it_Fashion_93.01acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "resnet20_model = torch.quantization.QuantWrapper(resnet20_model)\n",
        "B=8\n",
        "resnet20_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(resnet20_model, inplace=True)\n",
        "\n",
        "resnet20_model.to(\"cpu\")\n",
        "test(resnet20_model, fashion_testloader, cuda=False)\n",
        "resnet20_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(resnet20_model, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw0jaFTS7qiS",
        "outputId": "116fd278-ec04-4bd0-af79-4b73ee6b8cee"
      },
      "outputs": [],
      "source": [
        "resnet56_fp_model = CifarResNet(BasicBlock,[9]*3)\n",
        "\n",
        "resnet56_fp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet56/FashionMNIST/Train Iteration 3/model/ResNet56_3it_Fashion_93.29acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "resnet56_model = CifarResNet(BasicBlock,[9]*3)\n",
        "\n",
        "resnet56_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/ResNet56/FashionMNIST/Train Iteration 3/model/ResNet56_3it_Fashion_93.29acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "resnet56_model = torch.quantization.QuantWrapper(resnet56_model)\n",
        "B=8\n",
        "resnet56_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(resnet56_model, inplace=True)\n",
        "\n",
        "resnet56_model.to(\"cpu\")\n",
        "test(resnet56_model, fashion_testloader, cuda=False)\n",
        "resnet56_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(resnet56_model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR1R8uNR7qiT",
        "outputId": "e3825ae3-51be-4a8f-a947-6731c1a480a8"
      },
      "outputs": [],
      "source": [
        "mobilenet_fp_model = MobileNetV2(width_mult = 1.4)\n",
        "\n",
        "mobilenet_fp_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/MobileNetv2_x1_4/FashionMNIST/Train Iteration 3/model/MobileNet_3it_Fashion_93.31acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "mobilenet_model = MobileNetV2(width_mult = 1.4)\n",
        "\n",
        "mobilenet_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thesis/MobileNetv2_x1_4/FashionMNIST/Train Iteration 3/model/MobileNet_3it_Fashion_93.31acc.pkl\",map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "mobilenet_model = torch.quantization.QuantWrapper(mobilenet_model)\n",
        "B=8\n",
        "mobilenet_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
        "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
        "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
        "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
        "torch.quantization.prepare(mobilenet_model, inplace=True)\n",
        "\n",
        "mobilenet_model.to(\"cpu\")\n",
        "test(mobilenet_model, fashion_testloader, cuda=False)\n",
        "mobilenet_model.to(\"cpu\")\n",
        "\n",
        "torch.quantization.convert(mobilenet_model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7I6i8HZ3bxR"
      },
      "outputs": [],
      "source": [
        "#ResNet20 DataLoaders\n",
        "\n",
        "test_20_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_test_0).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_20_0_loader = torch.utils.data.DataLoader(dataset=test_20_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_test_1).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_20_1_loader = torch.utils.data.DataLoader(dataset=test_20_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_test_2).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_20_2_loader = torch.utils.data.DataLoader(dataset=test_20_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_20_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_test_3).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_20_3_loader = torch.utils.data.DataLoader(dataset=test_20_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_20_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_train_0).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_20_0_loader = torch.utils.data.DataLoader(dataset=train_20_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_train_1).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_20_1_loader = torch.utils.data.DataLoader(dataset=train_20_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_train_2).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_20_2_loader = torch.utils.data.DataLoader(dataset=train_20_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_20_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_20_train_3).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_20_3_loader = torch.utils.data.DataLoader(dataset=train_20_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twU4O4UB3bxR"
      },
      "outputs": [],
      "source": [
        "#ResNet56 DataLoaders\n",
        "\n",
        "test_56_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_test_0).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_56_0_loader = torch.utils.data.DataLoader(dataset=test_56_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_test_1).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_56_1_loader = torch.utils.data.DataLoader(dataset=test_56_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_test_2).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_56_2_loader = torch.utils.data.DataLoader(dataset=test_56_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_56_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_test_3).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_56_3_loader = torch.utils.data.DataLoader(dataset=test_56_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_56_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_train_0).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_56_0_loader = torch.utils.data.DataLoader(dataset=train_56_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_train_1).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_56_1_loader = torch.utils.data.DataLoader(dataset=train_56_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_train_2).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_56_2_loader = torch.utils.data.DataLoader(dataset=train_56_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_56_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_56_train_3).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_56_3_loader = torch.utils.data.DataLoader(dataset=train_56_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TvUrJqY3bxS"
      },
      "outputs": [],
      "source": [
        "#MobileNet Dataset\n",
        "\n",
        "test_mob_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_test_0).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_mob_0_loader = torch.utils.data.DataLoader(dataset=test_mob_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_test_1).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_mob_1_loader = torch.utils.data.DataLoader(dataset=test_mob_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_test_2).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_mob_2_loader = torch.utils.data.DataLoader(dataset=test_mob_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "test_mob_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_test_3).type(torch.FloatTensor), torch.from_numpy(y_f_test_set).type(torch.LongTensor))\n",
        "test_mob_3_loader = torch.utils.data.DataLoader(dataset=test_mob_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "train_mob_0 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_train_0).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_mob_0_loader = torch.utils.data.DataLoader(dataset=train_mob_0, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_1 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_train_1).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_mob_1_loader = torch.utils.data.DataLoader(dataset=train_mob_1, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_2 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_train_2).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_mob_2_loader = torch.utils.data.DataLoader(dataset=train_mob_2, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "train_mob_3 = torch.utils.data.TensorDataset(torch.from_numpy(x_f_mob_train_3).type(torch.FloatTensor), torch.from_numpy(y_f_train_set).type(torch.LongTensor))\n",
        "train_mob_3_loader = torch.utils.data.DataLoader(dataset=train_mob_3, batch_size=64, num_workers=2, pin_memory=True, shuffle=False, collate_fn=custom_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDmaufNG3bxT",
        "outputId": "9ffb6af4-d75c-4ca4-eb1d-47c6fb071f4a"
      },
      "outputs": [],
      "source": [
        "print(\"FashionMNIST TEST SETS:\")\n",
        "\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_0_loader)\n",
        "print(\"ResNet20 0: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_1_loader)\n",
        "print(\"ResNet20 1: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_2_loader)\n",
        "print(\"ResNet20 2: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,test_20_3_loader)\n",
        "print(\"ResNet20 3: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_0_loader)\n",
        "print(\"ResNet56 0: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_1_loader)\n",
        "print(\"ResNet56 1: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_2_loader)\n",
        "print(\"ResNet56 2: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,test_56_3_loader)\n",
        "print(\"ResNet56 3: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_0_loader)\n",
        "print(\"MobileNet 0: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_1_loader)\n",
        "print(\"MobileNet 1: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_2_loader)\n",
        "print(\"MobileNet 2: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,test_mob_3_loader)\n",
        "print(\"MobileNet 3: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUbg9ExN3bxU",
        "outputId": "8c579877-cdf4-480d-d5e0-6049a8c6e9ce"
      },
      "outputs": [],
      "source": [
        "print(\"FashionMNIST TRAIN SETS:\")\n",
        "\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_0_loader)\n",
        "print(\"ResNet20 0: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_1_loader)\n",
        "print(\"ResNet20 1: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_2_loader)\n",
        "print(\"ResNet20 2: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "res20_1, res20_5 = evaluator(resnet20_model,train_20_3_loader)\n",
        "print(\"ResNet20 3: Top_1: {}% Top_5:{}%\".format(res20_1, res20_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_0_loader)\n",
        "print(\"ResNet56 0: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_1_loader)\n",
        "print(\"ResNet56 1: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_2_loader)\n",
        "print(\"ResNet56 2: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "res56_1, res56_5 = evaluator(resnet56_model,train_56_3_loader)\n",
        "print(\"ResNet56 3: Top_1: {}% Top_5:{}%\".format(res56_1, res56_5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_0_loader)\n",
        "print(\"MobileNet 0: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_1_loader)\n",
        "print(\"MobileNet 1: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_2_loader)\n",
        "print(\"MobileNet 2: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "mob_1, mob_5 = evaluator(mobilenet_model,train_mob_3_loader)\n",
        "print(\"MobileNet 3: Top_1: {}% Top_5:{}%\".format(mob_1, mob_5))\n",
        "print(test(mobilenet_model,train_mob_3_loader, cuda=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If1k4ASuRgtU",
        "outputId": "cc63a22f-b27b-4081-be80-85687c3d0a69"
      },
      "outputs": [],
      "source": [
        "print(\"FashionMNIST TESTSET EVALUATOR\")\n",
        "\n",
        "res20_top1, res20_top5 = evaluator_testset(resnet20_fp_model, fashion_testloader)\n",
        "print(\"ResNet20 Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(res20_top1, res20_top5))\n",
        "\n",
        "res20_top1, res20_top5 = evaluator_testset(resnet20_model, fashion_testloader)\n",
        "print(\"ResNet20 Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(res20_top1, res20_top5))\n",
        "\n",
        "\n",
        "res56_top1, res56_top5 = evaluator_testset(resnet56_fp_model, fashion_testloader)\n",
        "print(\"ResNet56 Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(res56_top1, res56_top5))\n",
        "\n",
        "res56_top1, res56_top5 = evaluator_testset(resnet56_model, fashion_testloader)\n",
        "print(\"ResNet56 Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(res56_top1, res56_top5))\n",
        "\n",
        "\n",
        "mob_top1, mob_top5 = evaluator_testset(mobilenet_fp_model, fashion_testloader)\n",
        "print(\"MobileNet Floating-Point Evaluation: Top 1: {}% Top 5: {}%\".format(mob_top1, mob_top5))\n",
        "\n",
        "mob_top1, mob_top5 = evaluator_testset(mobilenet_model, fashion_testloader)\n",
        "print(\"MobileNet Quantized Evaluation: Top 1: {}% Top 5: {}%\".format(mob_top1, mob_top5))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "O7TH2ujaUs_A",
        "_S3sPpkgUwmI",
        "zTh24FhMYC28",
        "KRSPVYrZa-n6",
        "vGBDJRVEccJA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
